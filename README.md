# Data-Science
Learn about the fundamentals of data science, ranging from statistics and systems engineering to data mining and machine learning.
Titre du cours : Les fondements de la data science

Description : Ce cours donne une vue d'ensemble de la data science moderne, également appelée science des données en français, et qui est la pratique consistant à collecter, explorer, modéliser et interpréter des données. Si le big data monopolise toutes les attentions, il existe bien d'autres domaines et concepts intéressants. Barton Poulson évoque ici des branches telles que la programmation, les statistiques, les mathématiques, le machine learning, les analyses de données, la visualisation et (oui) le big data. Il explique pourquoi les data scientists sont si demandés et décrit les compétences requises pour réussir dans les différents métiers. Il montre aussi comment collecter des données à partir de référentiels open source via des API web ou via le scraping, et présente certaines technologies (R, Python et SQL) ainsi que certaines techniques (machines à vecteurs supports et forêts aléatoires) d'analyse. À l'issue de cette formation, vous devriez mieux comprendre l'utilité de la data science pour obtenir des informations exploitables à partir des jeux de données complexes qui vous entourent.


***********************************************
Chapitre : Introduction
***********************************************


-----------------------------------------------
Vidéo : Bienvenue dans « Les fondements de la data science »
-----------------------------------------------
Heure de la note :    Texte de la note :             

            Bienvenue dans « Les fondements de la data science »
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Bonjour et bienvenue dans ce cours d’introduction à la data science. Nous allons faire un tour d'horizon de la data science, un champ grandissant qui allie codage, maths, statistiques et sens des affaires. D'abord, je vous présenterai le processus que suivent les projets ainsi que les rôles et compétences qu'ils impliquent. Puis, je vous montrerai comment obtenir des données depuis diverses sources, notamment via les API web et le scraping. Nous verrons aussi comment utiliser R, Python, la ligne de commande et même les tableurs pour manipuler des données. Nous étudierons également des techniques d'analyse, comme les machines à vecteurs supports et les forêts aléatoires. Par ailleurs, nous aborderons les techniques permettant de planifier, piloter et présenter vos projets afin de vous aider à vous lancer dans la data science et à tirer le meilleur parti des données qui vous entourent. Commençons donc cette introduction à la data science. 


-----------------------------------------------
Vidéo : Utiliser les fichiers d'exercice
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:00            Utiliser les fichiers d'exercice
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Vous pouvez télécharger les fichiers d’exercice sur votre bureau, par exemple, comme je l'ai fait ici. Chaque dossier contient un exercice associé à un script ou un texte. Par exemple, le chapitre sur les sources de données ou les API s'accompagne d'un script R. Vous y trouverez aussi un script IPython et des fichiers comprenant des données à analyser. Si vous suivez cette formation sur un appareil mobile ou un terminal numérique, aucun problème : observez simplement comment je les utilise. Commençons sans plus attendre. 


-----------------------------------------------
Vidéo : Aborder les bases
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:00            Aborder les bases
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Afin de tirer le meilleur parti de cette formation, il vaut mieux être familier du langage de programmation statistique R, que j'utilise dans la plupart de mes exemples de code. Connaître Python, SQL ou la ligne de commande est un plus, mais n’est pas nécessaire. Enfin, il est utile d'avoir déjà fait des analyses de données et quelques statistiques. Cela dit, cette formation n'est qu'un tour d'horizon qui s'adresse à tous les profils et à tous les niveaux d'expérience. 


***********************************************
Chapitre : 1. Découvrir la data science
***********************************************


-----------------------------------------------
Vidéo : Comprendre la demande
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:00            Comprendre la demande
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Pour commencer, nous allons parler de la demande en matière de data science. Mais, pour cela, il faut d'abord expliquer ce qu'est la data science. Selon une définition courante, sur laquelle nous reviendrons plus tard, il s'agit d'une combinaison de statistiques et de programmation appliquées. Maintenant, il existe quelques autres définitions possibles. J'en ai deux de mon propre cru. La première, c'est l'analyse de données hétéroclites. J'entends par là des données au format inhabituel, quelle que soit leur source ou leur composition, et qui vous apportent les réponses clés dont vous avez besoin. La seconde, encore plus succincte, c'est l'analyse inclusive. J'entends par là une analyse qui inclut tout type de données. Tout ce que vous pouvez utiliser pour obtenir des informations exploitables. Cela semble bien peu de chose, comme ça, tout seul. Mais regardez cet article extrait de la Harvard Business Review. « Scientifique des données : le métier sexy du XXIe siècle ». Il exige des qualités rares, particulièrement recherchées. De quelles qualités s'agit-il ? De celles qui permettent de traiter des données non structurées – par exemple, du texte de page web dans un format ouvert, des blogs, des photos et des vidéos – et de trouver à la fois un ordre, une signification et une valeur à ces données. Et la demande dans tout ça ? Pourquoi s'avère-t-elle aussi forte ? Tout simplement parce que ces qualités permettent de sonder les esprits et les comportements, ce qui donne un avantage concurrentiel. Le besoin de data science est donc grandissant. Le McKinsey Global Institute a fait une projection de la demande pour l'année 2018. Voici ce qu'il en a conclu. Il y aurait une pénurie de compétences en données en 2018. Par exemple, rien qu'aux États-Unis, il viendrait à manquer de 140 à 190 000 talents dotés de profondes connaissances analytiques. Il est question ici des data scientists, autrement dit des techniciens qui manipulent des données au quotidien. Plus frappant encore, les entreprises rechercheraient 1, million de managers qui s'y connaissent en données et peuvent en faire un usage commercial, en sachant les placer dans leur contexte et en tirer des éléments concrets. C'est une demande énorme. Pour finir, un mot sur les salaires pratiqués dans cette branche. Voici le classement des métiers les mieux payés aux États-Unis, établi par le magazine U.S. News & World Report. J'y ai ajouté celui de data scientist sur la base d'un rapport O'Reilly Media. Comme vous le voyez, le médecin figure en première position, suivi par le dentiste. Aux États-Unis, le scientifique des données arrive troisième, trois rangs au-dessus de l'avocat. Son salaire moyen est de 144 000 $, avec un salaire de base de 104 000 $. C'est extrêmement rémunérateur. Quelques conclusions s'imposent à l'issue de cette brève introduction. Un, la data science jouit d'une forte demande. Deux, il faut aussi bien des spécialistes, autrement dit des techniciens de la data science, que des généralistes, autrement dit des managers orientés contexte qui appliquent les résultats. Trois, cette branche paie très bien. Ces arguments brossent un portrait attrayant de la data science et nous allons justement nous y intéresser de plus près. 


-----------------------------------------------
Vidéo : Découvrir le diagramme de Venn
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:00            Découvrir le diagramme de Venn
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Lorsqu'on essaie de définir le champ de la data science, l'un des meilleurs moyens de procéder est ce qui s'appelle le diagramme de Venn. Cet outil, créé en se compose de trois disques : séparément, chacun représente un domaine et, ensemble, ils forment la data science. Le premier, en haut à gauche, est celui du piratage ou de la programmation informatique : la capacité à récupérer et manipuler des données. À sa droite, celui des maths et statistiques : la capacité à donner un sens aux données. Et en bas, troisième point crucial, celui de l'expertise significative : la familiarité d'un domaine d'application particulier. Maintenant, passons rapidement en revue ces trois composantes. Les compétences en piratage permettent de collecter et préparer des données. Celles-ci sont souvent stockées dans des formats inhabituels, inadaptés aux lignes et aux colonnes des tableurs ou des bases de données. Le piratage ou la programmation requiert donc une grande créativité. Autrement dit, le codage informatique est une activité créative. En data science, chaque nouveau projet s'accompagne en effet de nouveaux défis. Passons aux mathématiques ou maths et statistiques. L'important ici n'est pas d'être un expert international en la matière, mais plutôt de savoir choisir une procédure utile afin de trouver des réponses aux questions qui vous intéressent et de savoir diagnostiquer les problèmes. La data science fait notamment naître le besoin d'élaborer et d’améliorer les procédures, si nécessaire, afin de relever de nouveaux défis. L'expertise significative consiste à comprendre, indépendamment de votre champ d'activité, ce qui a de la valeur. Par exemple, quels sont vos buts particuliers ? Quel est votre outil de travail ? Vous devez cerner vos buts. Vous devez connaître les méthodes et, surtout, les contraintes de votre domaine. Il existe des possibilités, mais aussi des impossibilités, qui vous aideront à encadrer votre analyse afin de la mettre en œuvre en toute facilité. Notre diagramme de Venn comporte donc ces trois disques qui, ensemble, forment la data science, mais il comporte aussi, de par sa composition, trois intersections de deux disques chacune. La première correspond à la zone du machine learning, la deuxième est la zone des recherches traditionnelles et la troisième est, selon Drew Conway, la zone de danger, sur laquelle nous reviendrons. Passons ces intersections en revue. Un, le machine learning. L'idée est que vous pouvez établir un modèle prédictif de type boîte noire. Il suffit de connaître les variables que vous souhaitez utiliser ainsi que ce que vous essayez de prédire pour créer le modèle correspondant. Deux, les recherches traditionnelles. Elles sont possibles, car dans la plupart des champs traditionnels, les données et les analyses sont structurées et affichent une sorte de continuité. Trois, selon Drew Conway, la zone de danger. Ce n'est probablement pas un vrai danger, car ceux qui allient capacité de programmation informatique et expertise significative font sans doute aussi des maths et des statistiques au passage. Que peut-on conclure de tout ça ? Un, la data science combine trois domaines : le piratage, les mathématiques et l'expertise. Deux, divers talents sont requis. Vous devez savoir faire beaucoup de choses différentes de façon raisonnablement satisfaisante dans le cadre de la data science. Trois, il existe par conséquent beaucoup de rôles différents dans cette branche. Les scientifiques des données affichent une multitude de compétences, de formations et d'orientations différentes, qui font l'objet de la vidéo suivante. 


-----------------------------------------------
Vidéo : Définir la séquence des données
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:00            Définir la séquence des données
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Le pilotage d'un projet de data science obéit systématiquement à une série d'étapes bien précises. C'est la séquence scientifique des données. Sachez qu'il existe quatre catégories de tâche. Un, les tâches de planification. Deux, les tâches de préparation des données. Trois, les tâches de modélisation ou d'analyse statistique des données. Quatre, pour conclure, les tâches de suivi. Nous allons les passer en revue rapidement. D'abord, phase 1, la planification. Quatre grandes tâches lui sont associées. Petit un, tout simplement définir des objectifs, autrement dit les buts que vous visez, afin de pouvoir concentrer vos efforts et savoir quand vous avez terminé. Petit deux, organiser les ressources. Par exemple, les types de données disponibles et les types de machine disponibles. Mais aussi, plus important encore, les personnes disponibles et leur temps disponible. Petit trois, une fois votre équipe de data science constituée, il faut coordonner ses membres. Cette tâche sociale s'avère essentielle à la réussite du projet. Enfin, petit quatre en matière de planification, il faut programmer le projet. Tout projet de data science étant en général collaboratif et étudié pour un client, c'est un point capital auquel il convient de prêter attention. On doit commencer par obtenir des données. Elles peuvent provenir de différentes sources. Cette tâche est susceptible d'être très créative. Ensuite, on nettoie. Autrement dit, on rend les données compatibles avec le programme utilisé, en s'assurant qu'elles ne comportent aucune erreur ni anomalie et qu'elles sont aussi bien valables que fiables. Petit sept, on explore les données. On regarde un peu les associations. Puis, petit huit, on affine les données. On choisit les cas d'utilisation à inclure. On sélectionne les variables à intégrer. On crée aussi les fonctionnalités souhaitées, qui complètent le contenu dont on dispose pour la phase suivante de la séquence scientifique des données. Ensuite vient la phase 3, la modélisation ou l'analyse des données. Petit neuf, on crée un modèle, ou même plusieurs modèles. L'étape suivante, petit dix, consiste bien sûr à valider chaque modèle. Autrement dit, il faut veiller à ce qu'il soit exact et généralisable sans trop de problèmes. On essaie d'estimer son exactitude et son intérêt réel dans notre quête de réponse à la question qu'on se pose. Petit douze, on affine le modèle. En fonction des évaluations effectuées, on peut vouloir apporter quelques modifications afin de rendre le modèle aussi informatif et aussi facile à mettre en œuvre que possible. Enfin, phase 4, le suivi, qui nous fait ressortir de la sphère purement technique. Il s'agit de présenter le modèle. En général, on expose les résultats de nos analyses devant le client concerné de telle sorte qu'il puisse non seulement y voir clair, mais aussi savoir quoi en faire. Après quoi, on déploie le modèle. Si vous avez élaboré un modèle prédictif destiné, par exemple, à un site d'e-commerce, vous devrez le placer sur le serveur de sorte qu'il soit alimenté en données clients afin de vous permettre d'établir des prédictions. Ensuite, il faut souvent revoir le modèle afin d'en assurer l'évolutivité. L'élaboration s'est faite avec un jeu de données, mais la mise en œuvre doit se faire avec un autre et il faut veiller à ce que tout fonctionne bien. Enfin, même si cela peut sembler tout bête, il est capital d'archiver l'intégralité des actifs utilisés. Notamment les jeux de données, des plus bruts aux plus propres, l'analyse finale, le code, les présentations et les commentaires. Ainsi, vous pourrez retracer vos actions passées, le client comprendra tout et, s'il fallait reprendre l'analyse afin de recalculer le résultat, ce serait possible. Que peut-on conclure de cette vue d'ensemble de la séquence scientifique des données ? Il ne s'agit pas seulement de technique. Les deux phases centrales, la deux et la trois, sont techniques, mais tout le reste de la data science va bien au-delà. Notamment les compétences contextuelles, qui s'avèrent essentielles à la réussite des projets. Enfin, la diversité se trouve favorisée. Le bon fonctionnement de la data science requiert tant d'éléments et tant de points de vue différents qu'il est capital de disposer de vue différents qu'il est capital de disposer de collaborateurs aux profils variés, de collaborateurs aux profils variés, capables de voir les choses sous tous leurs angles. capables de voir les choses sous tous leurs angles. 


-----------------------------------------------
Vidéo : Explorer les différents rôles
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:00            Explorer les différents rôles
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
La data science recouvre plusieurs rôles différents, dont chacun apporte aux autres les compétences et informations qui lui sont propres. Parlons d'abord des ingénieurs et développeurs. Ils se concentrent sur le matériel et les logiciels nécessaires à la data science. Ils constituent le back-end sur lequel tout repose. Ce sont les ingénieurs des données, développeurs et administrateurs de base de données. Passons aux spécialistes du Big Data. Ils ont une solide formation en informatique ou en mathématiques et s'intéressent beaucoup au machine learning, par exemple. Ensuite, les chercheurs. Ils mettent l'accent sur un domaine spécifique de recherche et sont souvent nettement plus versés dans le côté statistique de la data science que dans le côté back-end, matériel ou informatique de la data science. Puis, les analystes. Ils se concentrent sur les tâches quotidiennes, qui comprennent les analyses web, l'exploitation d'une base de données SQL et les visualisations de données. Voyons un peu les commerciaux. Ils gèrent les projets de data science et, surtout, ils définissent les questions commerciales auxquelles ils sont censés répondre, tout en aidant les autres à comprendre, interpréter et mettre en œuvre les solutions trouvées. Malgré leur orientation commerce ou management, ils doivent tout de même savoir parler le langage des données. Ils doivent cerner l'utilisation des données et leur signification ainsi que leurs points forts et leurs points faibles pour être en mesure de travailler correctement. Il y a aussi les entrepreneurs qui lancent ce qu'on appelle des start-up de données. Ils ont souvent besoin de créativité lors de la planification et du pilotage de leurs projets, qui doivent être pertinents sur le plan commercial. Enfin, n'oublions pas les data scientists polyvalents, qui, en théorie, sont capables de traiter chaque composante de la data science au maximum de leurs capacités. Ces professionnels sont extrêmement rares, d'où leur surnom de « licornes » qui évoque des créatures mythiques aux pouvoirs magiques. Toutefois, lorsqu'on arrive à recruter un data scientist polyvalent, on fait des progrès considérables et on travaille plus efficacement à répondre à des questions clés. Que peut-on conclure de tout ça ? La data science recouvre divers rôles. Elle implique un grand nombre de collaborateurs différents, qui contribuent tous à leur propre façon à la réussite du projet. Ensuite, chaque rôle se caractérise par des objectifs et des talents différents. L'administrateur d'une base de données ne fait pas la même chose que le chef d'une start-up de données, qui ne fait pas la même chose que le manager d'une grande structure spécialisée. Enfin, chaque rôle opère dans un contexte distinct qui se caractérise lui aussi par des finalités, des motivations, des méthodes, des contraintes et des significations différentes. 


-----------------------------------------------
Vidéo : Construire une équipe
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:00            Construire une équipe
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
La data science est souvent un sport d'équipe. C'est un effort collaboratif. Vous avez donc besoin de réunir plusieurs personnes aux capacités complémentaires sur votre projet. Déterminez d'abord les compétences requises, puis constituez ou étoffez votre équipe en conséquence. Illustrons cela par un exemple. Imaginons que nous ayons un data scientist prénommé Anthony, très fort en visualisation. Il s'en sort bien en programmation, mais a peu d'expérience analytique. Plus précisément, si nous demandions à Anthony de s'attribuer un score de zéro à dix pour chacune des cinq compétences requises, voici à quoi son profil pourrait ressembler. La ligne verticale grasse au niveau du huit marque le niveau nécessaire pour bien exécuter le projet. Elle est largement dépassée pour le graphisme, mais ce n'est pas la seule composante requise pour exécuter le projet comme il le mérite. Heureusement, Anthony a une collègue. Imaginons que cette autre personne s'appelle Abby. Elle a une solide expérience commerciale et s'en sort bien avec les aspects techniques des recherches, mais a peu d'expérience en visualisation. Voici à quoi ressemblerait le profil d'Abby : elle aurait huit en commerce, ce qui est le niveau requis, et atteindrait un très honorable score de cinq en programmation et en statistiques. Les autres points laisseraient à désirer. Comme Anthony, elle n'a pas le niveau partout, mais dispose néanmoins de nombreux atouts requis pour bien exécuter le projet. Une solution consisterait à réunir ces deux personnes, Anthony et Abby, afin de les faire travailler en binôme. Nous pourrions alors établir leur profil combiné, qui additionne leurs points forts sur un même graphique et, comme vous le voyez, le critère du score de huit sur dix serait largement satisfait pour les cinq compétences requises. Que pouvons-nous en conclure ? Un, qu'il est rare qu'une seule personne concentre toutes les compétences au niveau requis pour un projet. Deux, que le travail d'équipe est capital en data science, car il faut trouver des personnes compétentes et capables de collaborer de façon productive. 


***********************************************
Chapitre : 2. Définir le champ d'étude
***********************************************


-----------------------------------------------
Vidéo : S'initier au big data
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:00            S'initier au big data
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Ce qu'il faut savoir faire avec la data science, c'est bien la distinguer d'autres champs connexes. Pour commencer, nous allons la comparer au Big Data, car ce terme est souvent employé à tort dans le même sens. Cette confusion s'explique dans le cas suivant. Regardons le diagramme de Venn de la data science. Nous avons vu précédemment qu'il comportait trois domaines ou talents différents. Le piratage, autrement dit la programmation et le codage, les maths et les statistiques ainsi que l'expertise significative. Les trois ensemble forment la data science. Maintenant, regardons le diagramme de Venn du Big Data. C'est vrai qu'il comporte aussi trois disques caractéristiques, le Big Data se trouvant ici à l'intersection de ce qu'on appelle les trois V, à savoir le large volume des données, la grande vitesse à laquelle elles arrivent et leur variété considérable. C'est la conjonction de ces trois V qui génère du Big Data. Pour mieux distinguer les deux champs, je vais vous montrer mon propre diagramme de Venn à deux disques. Je l'appelle le diagramme de Venn du big data et de la data science, avec le big data à gauche, la data science à droite et la science du big data au centre. Cela montre que les deux champs se chevauchent en effet. Mais leurs concepts restent distincts. Étudions de plus près les points communs et les différences qui existent entre les deux. D'abord, le Big Data sans la data science. Le machine learning et les comptes de mots, par exemple. Ce sont des tâches simples sur le plan conceptuel qui se compliquent sur le plan pratique, mais qui ne nécessitent pas forcément beaucoup d'opérations mathématiques ou statistiques. En revanche, elles requièrent une combinaison d'au moins deux talents. La data science nécessite au moins deux domaines de son diagramme de Venn à trois disques. Puis, la science des données sans le Big Data. Avec les jeux de données génétiques, par exemple. Ils sont énormes, mais plutôt statiques et se présentent dans un format cohérent. Selon le cas, ils peuvent donc être considérés ou non comme du Big Data. C'est pareil pour les données capteur en streaming, qui sont très structurées, mais dont on ne conserve pas de gros volumes. De même pour la reconnaissance faciale. Le critère de la variété peut être au rendez-vous, mais avec un nombre de photographies très restreint. La troisième combinaison se trouve au centre. C'est la science du big data. C'est ce qu'on obtient quand le big data associe les trois V, à savoir volume, vitesse et variété. Il faut alors avoir tout l'arsenal de la science des données – à savoir programmation, statistiques et expertise – pour faire en sorte que tout fonctionne. Nous pouvons tirer trois conclusions de tout cela. Le big data n'est pas la science des données. Ces termes, souvent considérés comme synonymes, ne sont pas interchangeables. Toutefois, ils affichent des finalités et des techniques communes. Bon nombre de personnes savent y faire dans les deux champs. Enfin, la branche de la science du big data combine les défis et les talents de ces deux champs. 


-----------------------------------------------
Vidéo : Aborder la programmation
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:00            Aborder la programmation
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Un autre champ semble parfois chevaucher celui de la data science : celui de la programmation ou du codage informatique. Pour en donner une définition très schématique, cela consiste à donner des instructions à un ordinateur pour qu'il accomplisse une tâche. Cela peut induire une logique si/alors et des sauts. Mais cela peut être tout simple. Par exemple, dans Python version 2, il suffit de faire une impression d'écriture en saisissant hello world entre guillemets. On donne des instructions et l'ordinateur réagit en conséquence. La programmation pour des données est un peu différente. Reprenons l'exemple des comptes de mots, que j'ai déjà mentionné : il s'agit de prendre un texte volumineux et de le diviser en mots afin de compter les occurrences respectives de chacun. Cela indique la fréquence relative de chaque ensemble de mots. Mais des statistiques s'imposent pour les données variables et incertaines. Dans ce cas, il peut être utile de s'intéresser aux outils utilisés en programmation. J'ai pris ce classement des 12 meilleurs outils de programmation chez l'IEEE. On y trouve Java, C, C++, etc. Ces outils sont de type généraliste. R se classe ici à la sixième place. C'est un langage de données. Comparons avec les outils dédiés à la science des données. Voici, selon KDnuggets, les logiciels d'analyse, d'exploration et de science des données les plus utilisés en 2015. R se hisse ici à la toute première place. Java, C et C++ ne figurent pas du tout sur cette liste. Cela montre bien que, malgré leurs approches communes, ces deux champs utilisent des outils différents et forment donc deux niches aux écosystèmes distincts, l'un fondé sur les ordinateurs et l'autre fondé sur les données. Il y a plusieurs conclusions à en tirer, notamment que la science des données recouvre la programmation, qui est l'un des trois points du diagramme de Venn, et que toutes deux partagent bon nombre d'outils et de pratiques. Python est commun aux deux, R est commun aux deux et JavaScript est commun aux deux. Mais la science des données recouvre aussi les statistiques, qui s'accompagnent d'autres outils. Le contexte est donc différent, avec d'autres outils pour résoudre d'autres problèmes et obtenir d'autres types d'information. 


-----------------------------------------------
Vidéo : Comparer avec les statistiques
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:00            Comparer avec les statistiques
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Penchons-nous maintenant sur la relation entre data science et statistiques. La science des données et les statistiques sont souvent perçues comme deux disciplines distinctes, ce qui peut générer de la confusion, car il n'est pas rare qu'on les croie complètement dissociées, sans aucun point de contact entre elles. Or, on pourrait se demander si, puisque la science des données requiert impérativement des statistiques, elle ne pourrait pas être plutôt un sous-domaine des statistiques. Les deux champs affichent en effet des procédures communes. La data science serait alors un sous-domaine des statistiques, ce qui donnerait ce type de diagramme. Toutefois, si c'était le cas, alors tous les scientifiques des données seraient des statisticiens de formation qui se seraient spécialisés dans leur champ et nous savons que ce n'est pas vrai. Intéressons-nous à quelques points. Un, la formation. Peu de scientifiques des données sont des statisticiens. Ils ont plus souvent une formation en informatique, et les profils d'ingénieur, de physicien et d'astrophysicien sont également courants. Deux, les techniques qui diffèrent. Le machine learning et le Big Data sont deux composantes de la science des données peu utilisés dans les statistiques. Trois, l'environnement. La science des données induit d'autres contextes. Ce diagramme présente donc une autre façon d'envisager la relation entre ces deux disciplines. Et si c'étaient deux niches distinctes ? Elles seraient liées par leurs nombreuses pratiques communes, mais resteraient néanmoins différentes sur le plan écosystémique. En conclusion : les deux champs exploitent des données, mais affichent des différences en ce qui concerne leurs motivations, leurs finalités, leurs formations et leurs contextes. Vus sous cet angle, ils peuvent paraître distincts l'un de l'autre. 


***********************************************
Chapitre : 3. Utiliser la data science de façon éthique
***********************************************


-----------------------------------------------
Vidéo : Évaluer les questions éthiques
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:00            Évaluer les questions éthiques
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
La science des données soulève des questions éthiques capitales, qui découlent de l'analyse des données dans certains formats. C'est une activité créative en ce qu'elle exploite des données de façon quelque peu détournée. Cela pose quelques questions sérieuses. Premier sujet, la vie privée. La confidentialité est un enjeu majeur. Il s'agit des informations à ne surtout pas partager, anonymes ou non. C'est important, car la science des données s'appuie souvent sur des sources non conçues pour le partage. L'anonymat est un enjeu clé en matière d'éthique des recherches et doit être soigneusement pris en considération. Les différentes règlementations, comme par exemple le RGPD, rendent désormais l'identification personnelle bien plus difficile. L’anonymité des données peut varier selon que l’on parle de données publiques ou de données propriétaires, par exemple celles qui vous sont fournies par un client, et selon les règlementations en vigueur. Troisième sujet, dans un autre genre, les droits d'auteur et le copyright. Le scraping, autrement dit l'extraction d'images ou bien de texte à partir d'un site web pour en retirer des données, est une pratique courante. Or, certaines de ces données peuvent violer un copyright. Si les images ou le texte en question sont protégés, vous pourriez risquer gros en accédant à ces données sans l'autorisation du détenteur du copyright. Je terminerais par deux points que vous n'aviez peut-être pas encore envisagés. Un, le parti pris potentiel. Les algorithmes utilisés en science des données sont, naturellement, neutres en eux-mêmes. Ils n'ont pas d'opinion. Cela dit, la neutralité des algorithmes dépend de la neutralité des règles et des données des programmeurs. Ensuite, deux, l'excès de confiance. Les analyses de données sont des simplifications. Toute analyse est une simplification. Il faut des intervenants humains. Sinon, certains croient que tous les résultats des algorithmes de machine learning sont systématiquement vrais et corrects, sans se rendre compte du parti qui a été pris et des problèmes d'interprétation qui en découlent. Le facteur humain reste vital. En conclusion : la science des données présente du potentiel et des risques. On le savait. Mais surtout, les analyses ne sont pas neutres et un jugement humain est donc toujours requis pour planifier et exécuter des projets de science des données ainsi que pour en interpréter les résultats. de science des données ainsi que pour en interpréter les résultats. 


***********************************************
Chapitre : 4. Déterminer les sources de données
***********************************************


-----------------------------------------------
Vidéo : Choisir les indicateurs
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:00            Choisir les indicateurs
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Lorsqu'on s'apprête à lancer un projet de data science, l'un des points à ne pas négliger consiste à choisir un indicateur ou une mesure de réussite. Cela en raison de l'orientation objectif. Il faut connaître son objectif afin de pouvoir déterminer quand on l'a atteint ou si on a généré une solution viable. Précisément, la data science est orientée action, dans un but de réalisation. Au contraire de la recherche universitaire, qui est orientée connaissance, dans un but de compréhension. Et je sais bien de quoi je parle. En data science, tout objectif doit être explicite pour canaliser nos efforts ainsi que pour savoir ce qu'on vise, si on est bien parti pour y parvenir et s'il faut corriger quoi que ce soit. De plus, un objectif clair profite au client. Enfin, cela vous profite en qualité d'analyste, Il faut avoir une orientation objectif, mais laisser tout de même la porte Certaines occasions peuvent se présenter, mais il faut toujours d'abord déterminer ce qu'on essaie d'accomplir. Parlons maintenant des indicateurs ou mesures spécifiques. Je vais vous présenter plusieurs façons de procéder. objectifs SMART (c'est un acronyme), précision de classification. Passons ces outils en revue. Les indicateurs de performance clés ou KPI. Ils sont signés David Parmenter. En général, ils doivent être de nature autre que financière, et non relever des coûts ou du chiffre d'affaires, qui sont plus des fins que des moyens. mensuel ou continu, et axés PDG, car ce sont les PDG qui pilotent les KPI. Les indicateurs doivent être simples afin que toutes les personnes concernées les comprennent. Ils sont à fixer par équipe pour que les membres en partagent la responsabilité. Impact significatif. Cela signifie que tout indicateur doit affecter plus d'un résultat important : la rentabilité et la portée commerciale ou le délai de fabrication et le nombre de défauts. Les objectifs SMART. Cet acronyme signifie spécifiques, mesurables, assignables (il faut les attribuer), réalistes (il faut les atteindre avec les ressources données) et temporellement définis (il faut les assortir d'une échéance précise). Il s'avère d'ailleurs important, pour les projets de science des données, de définir une fenêtre d'exécution ou des créneaux particuliers. La sphère de la recherche utilise des tables de classification, qui indiquent si le test effectué se révèle positif ou négatif et si la condition posée est présente ou absente. Cela donne des combinaisons « vrais positifs », « faux positifs », etc. Sur la base de ces informations, il est possible de calculer des statistiques récapitulatives, fréquemment utilisées pour évaluer les classifications. Elles recouvrent la sensibilité – autrement dit, si la condition est bien présente, la probabilité d'obtenir une réussite mesurée –, la spécificité, consistant à éviter les faux négatifs, la valeur prédictive positive et la valeur prédictive négative. Nous pouvons en tirer trois conclusions. Un, il existe un effet sensibilisateur. Un, il existe un effet sensibilisateur. Lorsqu'on mesure un résultat, on se donne les moyens de réagir Lorsqu'on mesure un résultat, on se donne les moyens de réagir concrètement en conséquence. concrètement en conséquence. Deux, la sensibilisation contribue à la qualité des produits ou bien Deux, la sensibilisation contribue à la qualité des produits ou bien des services que vous fournissez. des services que vous fournissez. Trois, il faut effectuer vos mesures avec soin et précaution. Trois, il faut effectuer vos mesures avec soin et précaution. 


-----------------------------------------------
Vidéo : Rassembler les données existantes
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:00            Rassembler les données existantes
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Lorsqu'on cherche des sources, le plus facile est de s'intéresser d'abord aux jeux de données existants. Le plus simple est encore de puiser dans les données internes, mais sachez qu'il existe aussi des données ouvertes et des données tierces. Nous pèserons le pour et le contre de chaque catégorie. Les données internes peuvent être, par nature, faciles d'accès. Elles sont en général au bon format et, avec de la chance, consignées correctement. En revanche, elles ne sont pas toujours soumises à un contrôle qualité aussi strict que celui des données qui sont destinées à des tiers. De plus, les entreprises les assortissent parfois de stratégies ou de restrictions d'utilisation. Voyons ensemble le pour et le contre des données internes. Déjà, leur accès devrait être facile, rapide et gratuit, car elles sont la propriété de votre entreprise. Avec de la chance, elles suivent un format standardisé, car elles obéissent aux règles et stratégies adoptées par votre entreprise. Dans le meilleur des cas, les membres de l'équipe qui a initialement collecté et traité ces données sont encore là. Cela facilite la vie lorsqu'on cherche des réponses et des informations. Des éléments d'identification peuvent aussi être disponibles, autrement dit vous pouvez savoir de qui ou de quoi émanent ces données. Mais il y a du contre. Déjà, les données dont vous avez besoin peuvent ne pas exister. Il se peut que personne n'ait pensé à les collecter. Leur consignation peut laisser à désirer. Il arrive souvent que les entreprises collectent des données internes dans un certain but immédiat, puis les stockent dans un coin sans qu'on puisse facilement s'y retrouver plus tard. De plus, la qualité des données est parfois douteuse. Si on n'est pas conscient des partis pris ou des erreurs de codage, il devient difficile d'obtenir des résultats fiables. Pourquoi ne pas choisir des données ouvertes ? Elles sont préparées et libres de droits. Il s'agit de données officielles, mais également parfois professionnelles. Les données scientifiques viennent aussi accroître leur nombre. Côté pour, on peut alors obtenir d'énormes jeux de données – de plusieurs téraoctets, voire pétaoctets, qui valent des millions de dollars –, on couvre une large palette de sujets, avec divers historiques et différentes tendances, et on bénéficie de données qui sont souvent bien formatées et bien consignées. Côté contre, les échantillons de données ouvertes peuvent être biaisés, par exemple s'ils sont limités à une certaine zone géographique ou aux personnes disposant d'une connexion Internet, ce qui peut poser des problèmes d'interprétation. De plus, le sens des variables n'est pas toujours clair. Les membres de l'équipe ayant collecté les données n'étant pas disponibles, il est difficile de comprendre pourquoi ou comment ils ont défini certaines variables. Il arrive également que des données ouvertes vous imposent de partager vos analyses. Qui dit données ouvertes dit analyses ouvertes. Cela peut poser problème et mieux vaut prêter attention à ce point avant de vous lancer. Enfin, la nature ouverte de ces données peut entrer en conflit avec le besoin de confidentialité. Enfin, il existe les données tierces, dites « à la demande » ou DaaS. Détenues par les courtiers en données, elles sont très nombreuses et couvrent une large palette de sujets. Il arrive souvent que les courtiers traitent ces données afin d'en déduire des conclusions qui peuvent vous faire gagner un temps précieux. Côté pour, les données tierces peuvent faire économiser du temps et des efforts. Il s'agit souvent de données individuelles, assorties de synthèses et d'inférences. Côté contre, les données tierces peuvent se révéler très onéreuses. Elles restent parfois aussi à valider. Il faut donc encore s'assurer qu'elles signifient bien ce qu'on pense. Mais le plus important, c'est sans doute qu'elles déplaisent à beaucoup de personnes. En conclusion : faites attention lorsque vous interprétez des données, car il se peut qu'elles soient biaisées ou que, faute d'avoir participé à leur collecte, vous ne puissiez pas savoir exactement ce qu'elles signifient. Cela dit, les jeux de données existants peuvent fournir des volumes d'informations considérables et sensiblement accélérer les projets de data science. 


-----------------------------------------------
Vidéo : Découvrir les API
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:00            Découvrir les API
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Un bon moyen de collecter des données pour un projet de science des données consiste à passer par une API. Cet acronyme d'Application Programming Interface désigne une interface de communication entre programmes informatiques. Outre les API présentes dans les systèmes d'exploitation, il en existe plusieurs qui permettent d'accéder aux données web et de les extraire de diverses sources. Les API les plus couramment utilisées en science des données sont de type REST, acronyme de Representational State Transfer, qui fait référence au style d'architecture logicielle du web. Les API REST permettent d'accéder aux données d'une page web via le protocole HTTP et de les extraire au format JSON, autrement dit Java Script Object Notation. L'avantage des API REST, c'est la possibilité d'envoyer des données d'une page web vers d'autres programmes aux fins d'analyse. Et elles sont agnostiques en matière de langage. Les plus populaires sont les API sociales. Par exemple, Facebook a ce type d'interface. Twitter dispose d'une API qui permet d'extraire ses données. Google Talk est une API très populaire. Foursquare et SoundCloud sont deux autres exemples connus. Il est possible de créer des applications qui se connectent directement à ces réseaux sociaux et, parfois, d'en extraire des données dans le cadre d'un projet. Il existe aussi des API visuelles. Par exemple, Google Maps, YouTube ou encore AccuWeather, qui est très courante, tout comme Pinterest et Flickr. Maintenant, je vais vous montrer une API toute simple basée sur R. Je vais extraire des données du site web ergast.com, spécialisé dans les courses automobiles. Voici la page d'accueil sur laquelle on arrive. Les informations sont succinctes, mais le lien dirige vers cette page. Il s'agit d'une API de développeur dédiée aux Grands Prix de Formule 1 de 1950 à 2014. Il suffit de créer une URL. Ici, je vais rechercher tous les vainqueurs des Grands Prix de l'année 1957. Dans ergast.com, l'API, on veut la Formule 1 de On obtient, au format JSON, la liste de tous les vainqueurs de l'année. Maintenant qu'on connaît l'URL, on va dans R. Je vais utiliser ici le package jsonlite, qui permet d'accéder aux données JSON à partir du web, ainsi que sa dépendance, curl, pour les URL. Si vous ne les avez pas déjà, utilisez la commande install.packages et ajoutez la commande require pour charger jsonlite. Curl se charge par défaut. Je vais agrandir un peu ce volet. Ensuite, je reprends mon URL de recherche, qui se trouve juste ici, et je la fais précéder de la commande fromJSON au sein de l'objet f1, abréviation de Formule 1. Pour vous donner une idée du contenu, c'est un objet liste. Autrement dit, il est très volumineux et contient des tonnes de texte. Mais il est structuré, comme nous l'avons vu. D'ailleurs, je vais maintenant développer cet objet, en agrandissant le volet, pour vous montrer qu'il a une structure emboîtée, même s'il est plutôt difficile de déchiffrer tout cela sous cette forme. Je vais prendre une partie de cette liste de l'objet f1, descendre au niveau des données des courses automobiles, puis descendre au niveau des tables des courses, etc., jusqu'au niveau du pilote, le tout au sein de l'objet dr, abréviation de l'anglais driver. Je veux les en-têtes de colonne correspondants. Il y a sept colonnes ; j'en retiens quatre que j'agence dans un ordre différent, et voici les cinq premiers vainqueurs dans ce jeu de données. L'Argentin Juan Fangio, qui a remporté le premier Grand Prix de l'année, est l'un des meilleurs pilotes de l'histoire. Nous avons donc ici un fabuleux moyen d'envoyer des données JSON structurées du web vers R afin de les soumettre à nos analyses personnelles. Quelles conclusions pouvons-nous tirer de cette petite démonstration ? Un, les API facilitent l'accès aux données web structurées. Deux, il est possible d'extraire ces données afin de les envoyer vers un programme d'analyse. Trois, les API constituent d'excellents outils pour ce qui est de récupérer des données structurées et de les intégrer à des analyses afin d'obtenir les informations recherchées. 


-----------------------------------------------
Vidéo : Recourir au scraping
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:00            Recourir au scraping
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Un autre bon moyen de collecter des données consiste à recourir au scraping. C'est important, car toutes les données ne sont pas au format JSON. Toutes ne sont pas facilement disponibles, ce qui oblige à élaborer une stratégie d'accès créative. Par exemple, une page web peut intégrer du texte HTML, des tables comme dans Wikipédia, des PDF ou encore des fichiers image, vidéo ou audio. N'oublions pas les documents Word, les présentations PowerPoint et la multitude d'autres sources dont les données ne sont pas prêtes à l'emploi dans les programmes d'analyse tels que Python ou R. Avant d'aller plus loin, je vous conseille de faire attention au copyright et à la confidentialité. Ces données n'ont pas été créées pour vous et leur accès s'avère parfois restreint à juste titre. Vous devez connaître et respecter les limites imposées, en agissant toujours avec professionnalisme. Dans le cadre du scraping, deux options s'offrent à vous. Un, les outils tout faits. On a notamment import.io. C'est à la fois une URL et le nom d'un outil qui facilite grandement le scraping des pages web. ScraperWiki et Tabula font la même chose. Même dans Google Sheets, la commande =IMPORTHTML_ permet d'extraire les données d'une table, ce qui est aussi possible dans certaines versions d'Excel. Deux, les outils personnalisés. Il est possible d'utiliser R, Python, Bash – qui est la ligne de commande –, Java ou PHP. Chacun d'eux permet d'accéder à des données et de les manipuler afin d'être en mesure de les exploiter dans un format analysable. Donc, en conclusion : le scraping marche lorsqu'il n'existe pas d'API pour les données qui vous intéressent. De plus, vous pouvez utiliser des applications dédiées ou coder dans votre langage de prédilection. Enfin, les questions de copyright et de confidentialité comptent et sont à prendre en considération avant de vous lancer dans le scraping de données de page web. 


-----------------------------------------------
Vidéo : Créer des données
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:00            Créer des données
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Comment créer des données ? Il faut savoir quoi faire lorsque les données recherchées sont indisponibles. Il existe différentes méthodes envisageables, qui se distinguent par quelques facteurs bien précis. Par exemple, quel est votre degré d'intervention ? Est-ce que vous faites une simple observation ou plutôt une expérimentation, en créant une situation particulière ? De quelles données disposez-vous ? Sont-elles quantitatives ou numériques, comme celles utilisées en data science ? Sont-elles qualitatives, ce qui permet notamment d'analyser du texte, mais avec une autre approche ? Que dire de l'emplacement ? Plutôt en ligne ou en personne ? Toutes ces combinaisons sont possibles pour collecter des données si vous soignez la phase de conception. Prenons un premier exemple, celui des entretiens. L'idéal pour les nouveaux sujets ou publics. Ils peuvent s'avérer structurés ou non. Toutefois, en général, les entretiens prennent beaucoup de temps et nécessitent une formation spécialisée. Dans certaines situations, il peut se révéler utile de recruter un anthropologue ou un sociologue pour conduire ce type de recherche pour votre compte. Deux, les enquêtes. Les questions sont ouvertes ou fermées. Les enquêtes sont faciles à envoyer en masse en ligne, mais leur conception est tout un art : il faut prêter attention à la formulation des questions et prendre garde aux échantillons biaisés afin d'obtenir les données les plus utiles pour atteindre les objectifs du projet. Trois, le tri de fiches. Les participants trient les sujets qui sont inscrits sur différentes fiches. C'est une activité utile pour définir des catégories et des hiérarchies. Cela permet aussi d'évaluer la convivialité d'un site web. Plusieurs types de tri de fiches sont envisageables, notamment un d'exploration pour déterminer comment vous y prendre et un de validation pour voir si la solution trouvée fonctionne. Quatre, les expériences. C'est là qu'intervient la manipulation. Il s'agit d'intervenir concrètement en créant une situation différente pour un groupe ou pour un autre. Certains ont peur du mot « manipulation », qui évoque pour eux la coercition et le lavage de cerveau. Ici, il s'agit de créer des conditions différentes. C'est d'ailleurs considéré comme idéal pour établir des inférences de causalité. En revanche, en général, l'expérimentation nécessite beaucoup de main-d'œuvre ainsi qu'une formation spécialisée extrêmement pointue. Il existe une exception à cette règle et c'est le test A/B, très courant sur Internet, qui consiste à avoir deux versions d'une page web ou bien d'un élément web et à assigner chaque version de façon aléatoire aux internautes. C'est très simple et rapide à faire. Que peut-on conclure de tout ça ? Collectez exactement les données requises. C'est votre but et, si elles n'existent pas, vous pouvez les créer. Les méthodes envisageables peuvent nécessiter une formation approfondie ou l'aide d'un consultant. Les recherches posent des questions éthiques capitales, surtout lors des collectes en direct et en personne. Pensez alors à solliciter l'avis d'un expert. 


***********************************************
Chapitre : 5. Explorer les données
***********************************************


-----------------------------------------------
Vidéo : Tirer parti des graphiques exploratoires
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:00            Tirer parti des graphiques exploratoires
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Une fois que vous disposez des données souhaitées, vous devez examiner leur contenu. Pour cela, rien ne vaut les graphiques exploratoires. Ils permettent de se familiariser avec les données, de confirmer les hypothèses associées à l'analyse prévue, de traquer les anomalies et de voir si vous trouvez des pistes inédites ou inattendues. Tout cela vous indique si l'analyse que vous prévoyez d'effectuer est adéquate et si vous allez pouvoir légitimement en tirer les conclusions que vous souhaitez. Mais pourquoi commencer par des graphiques ? Parce que, de par leur nature visuelle, ils sont riches en informations. intervenants humains, c'est-à-dire visuellement. Ils permettent aussi de repérer rapidement les distributions, les écarts et les aberrations. Tout cela peut avoir un impact significatif sur votre analyse. Commencez par des distributions simples, unidimensionnelles, autrement dit à une variable. Puis, passez aux distributions combinées, qui associent plusieurs variables. que les erreurs éventuelles. Faites attention aux manques : une valeur peut être absente. Parfois, cela ne veut rien dire du tout. C'est le cas des données manquantes complètement aléatoirement, qu'on peut ignorer. Il y a aussi les données manquantes aléatoirement, dont l'absence s'explique compte tenu des variables observables. Et il y a les données manquantes non aléatoirement, ou non-réponses non ignorables. Elles posent problème parce qu'elles induisent un certain parti pris. Maintenant, voici vos options en matière d'exploration. Un, vous pouvez essayer le codage. R est courant dans le domaine exploratoire. Python aussi. JavaScript, surtout pour les bibliothèques D3 (Data-Driven Documents), est parfait pour élaborer des graphiques exploratoires. Deux, il existe des applications spécialisées. Tableau est vraiment bien ; Qlik également. Excel peut aussi tout à fait convenir si vous savez y créer rapidement des graphiques, car vous pouvez les actualiser de façon dynamique. Trois, vous pouvez tout faire à la main, comme John Tukey, inventeur de l'analyse exploratoire. C'est une option qui permet de se familiariser beaucoup plus intimement avec les données. Voyons les graphiques à barres. Ils sont plutôt étudiés pour les catégories. Ils sont faciles à lire, surtout avec des valeurs en ordre décroissant. C'est le cas ici, le groupe le plus fréquent étant à gauche, contre l'axe des ordonnées. Et on peut les grouper pour voir les associations entre les variables. Autrement dit, les différences entre les groupes. Les boîtes à moustache sont parfaites pour les variables quantitatives, c'est-à-dire mesurées. Elles indiquent les valeurs de quartile – médiane, premier quartile, minimum, maximum – et les aberrations. C'est un de leurs intérêts majeurs. On peut aussi les grouper, et même afficher plusieurs variables à la fois, tant qu'elles relèvent d'échelles identiques ou similaires. Passons aux histogrammes. Ils sont, eux aussi, utilisés pour les variables quantitatives. Ils montrent la forme de la distribution. L'avantage est qu'il est possible de les superposer, comme je l'ai fait ici, afin de la comparer aux autres formes possibles. Ensuite, sur le plan des associations, il est utile de créer un diagramme de dispersion. Voire une matrice de diagrammes de dispersion. Celle-ci illustre l'association de trois variables quantitatives. Elle comprend également un histogramme pour chacune de ces variables. Je peux vous dire que ce type de matrice se lit plus facilement qu'un diagramme de dispersion en 3D. Lorsque vous passerez en revue ces graphiques, posez-vous les questions suivantes. Avez-vous ce qu'il faut pour atteindre le but visé ? Y a-t-il une quelconque anomalie ? Existe-t-il des exceptions ? Avez-vous relevé des erreurs ? Les graphiques exploratoires vous aideront à répondre à ces questions et à poser le cadre d'une analyse encore plus pertinente. Quelles conclusions tirer de tout cela ? Un, l'exploration est la première phase essentielle de toute bonne analyse. Deux, il faut employer une méthode simple et rapide. Veillez à opter pour un outil adapté, qui vous est familier et dont vous savez vous servir rapidement. vous savez vous servir rapidement. Trois, l'exploration graphique précède l'exploration numérique, Trois, l'exploration graphique précède l'exploration numérique, qui fait l'objet de la vidéo suivante. qui fait l'objet de la vidéo suivante. 


-----------------------------------------------
Vidéo : Utiliser les statistiques exploratoires
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:00            Utiliser les statistiques exploratoires
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Après le temps des graphiques exploratoires vient le temps des statistiques exploratoires, aussi appelées explorations numériques. Cela peut se résumer en une formule : les graphiques d'abord, les chiffres ensuite. Attention, les statistiques exploratoires consistent encore à explorer les données, et non à les modéliser. Par ailleurs, on utilise plutôt des estimations empiriques, c'est-à-dire basées sur un échantillon, On peut aussi voir l'effet des manipulations de données pour savoir comment elles réagissent aux variations. de statistiques exploratoires. Les statistiques robustes, le rééchantillonnage ou auto-amorçage ainsi que la transformation de données. Passons ces trois méthodes en revue. J'en ai des tonnes, très difficiles à déplacer. Ces statistiques s'avèrent stables en présence d'anomalies. Et moins sensibles aux aberrations, asymétries et aplatissements. Elles offrent un grand choix, notamment la moyenne tronquée, la médiane, la moyenne de Windsor, l'intervalle interquartile et l'écart absolu médian. Elles ne sont pas toujours faciles à réaliser. Les programmes dédiés ne les prennent pas en charge automatiquement. Toutefois, il existe des packages tels que R ou Python qui facilitent le processus. Voici un exemple avec des asymétries. Dans ce jeu de données, la plupart des valeurs sont très faibles et les aberrations montent très haut. Ce que je vais faire, c'est prendre la moyenne tronquée et ce qui s'appelle la moyenne de Windsor. La moyenne tronquée s'obtient en éliminant un certain pourcentage des données extrêmes, au-dessus de la barre. La moyenne de Windsor consiste à prendre à la place la valeur non aberrante la plus proche. Vous voyez que si on choisit 0 %, on ne constate aucun ajustement : la moyenne générale est de 1,24. Si on passe à 5 %, les moyennes commencent à baisser. La moyenne tronquée est encore plus basse. On élimine alors 10 % des extrêmes, puis 25 %. Enfin, quand on arrive à 50 %, on retrouve la médiane. C'est le résultat du milieu, qui est ici 1,01. Vous pouvez choisir la version qui vous semble la plus informative dans le cadre de votre propre analyse. Vous pouvez aussi ignorer ces statistiques, mais il est bon de savoir qu'elles existent. Deux, le rééchantillonnage ou auto-amorçage. Il s'agit d'estimer de façon empirique la variabilité de l'échantillonnage : au lieu de l'écart-type de la population, on obtient plusieurs écarts-types en échantillonnant les données afin d'estimer la variabilité. Le jackknife consiste à utiliser des sous-jeux de données aléatoires, en échantillonnant sans remplacement. Le bootstrap consiste, lui, à échantillonner avec remplacement. La permutation consiste à réagencer les cas entre différents groupes. De plus, l'idée de la validation croisée qui s'impose en machine learning est liée au rééchantillonnage comme moyen de vérifier la cohérence des résultats. Et, trois, la transformation. Il s'agit de trouver des fonctions lisses qui ne présentent pas de grosses irrégularités afin de préserver un certain ordre tout en utilisant l'intégralité du jeu de données. Cela sert souvent à rectifier des aberrations ou des lignes courbes dans les diagrammes de dispersion. Une méthode très courante est l'échelle des puissances de Tukey, en référence à John Tukey. Ici, par exemple, le troisième élément en partant du haut est X. C'est la valeur initiale, qui augmente si on l'élève au carré ou au cube et qui baisse si on calcule sa racine carrée ou son logarithme, etc. Voici à quoi cela ressemble sous forme graphique. Le jeu X est le troisième en partant de la droite. Il est assez symétrique. On voit bien que, au carré, la boîte descend et des aberrations apparaissent en haut et, au cube, encore plus. À l'extrême opposé, avec la réciproque de la racine carrée, on obtient un tout autre effet. Ne vous lancez pas là-dedans pour créer des aberrations, mais si certaines sont déjà présentes, cette méthode vous fera retrouver une forme plus symétrique permettant d'analyser l'intégralité du jeu de données. Voici ce qu'on peut retenir des statistiques exploratoires. Voici ce qu'on peut retenir des statistiques exploratoires. Un, elles montrent les données sous plusieurs angles. Un, elles montrent les données sous plusieurs angles. Deux, elles permettent d'évaluer la stabilité selon les circonstances. Deux, elles permettent d'évaluer la stabilité selon les circonstances. Trois, ce sont elles qui plantent le décor de la modélisation. Trois, ce sont elles qui plantent le décor de la modélisation. 


***********************************************
Chapitre : 6. Travailler avec la programmation
***********************************************


-----------------------------------------------
Vidéo : Découvrir le rôle des feuilles de calcul
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:41            Découvrir le rôle des feuilles de calcul
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Le premier sujet que je veux aborder sur le thème « programmation et science des données » est le rôle des feuilles de calcul. Même si les tableurs peuvent ne pas payer de mine, ils se révèlent souvent très pertinents pour mener à bien vos projets en cours. Et ce, pour plusieurs raisons. Un, les feuilles de calcul sont partout. Microsoft Excel est chargé sur presque tous les PC, Google Sheets est accessible depuis n'importe quel ordinateur et le tableur est le format préféré des clients. Comme ils vous fournissent leurs données dans des feuilles de calcul, vous devez pouvoir les exploiter sans problème afin de lancer votre analyse. Le format CSV, avec la virgule comme séparateur, permet d'effectuer des transferts de données entre tous les programmes existants. N'importe quel programme peut lire et écrire un fichier CSV. Cela permet de transférer des données sans difficulté. Les feuilles de calcul sont conviviales : certaines opérations y sont même plus simples quand dans d'autres programmes et je vous en donnerai quelques exemples bientôt. Laissez-moi d'abord vous montrer les résultats d'une enquête sur les logiciels utilisés en exploration de données. Dans cette liste, dressée par KDnuggets, vous voyez qu'Excel se classe cinquième ! Devant Hadoop et Spark, qui sont pourtant des outils spécialisés dans le big data. Voici pourquoi Excel reste prédominant parmi tous ces logiciels. Déjà, il a de nombreuses applications clés. Il convient au survol des données, qui désigne tout simplement la possibilité d'étudier les données dont on dispose en les faisant défiler aussi bien à l'horizontale qu'à la verticale. Il convient au tri des données. Il convient au réagencement manuel des lignes et colonnes. Il est très pratique sur ce point. Il convient aux opérations de recherche et remplacement si importantes en science des données. Le fait que les tableurs facilitent ces actions les rendent incontournables. Les feuilles de calcul ont aussi d'autres usages. Exemple : le formatage des données. La transposition des données, permettant de changer les lignes et colonnes de place. Le suivi des modifications, pour savoir quel membre de l'équipe a fait quoi. La création de tableaux croisés dynamiques, comme ici. Certains pensent même que les feuilles de calcul sont faites pour créer ces tableaux dans une optique d'exploration et de manipulation des données. De plus, les feuilles de calcul sont parfaites pour agencer un contenu à présenter ou partager. Cela dit, le fait que les feuilles de calcul offrent cette grande flexibilité est susceptible de poser problème lors des partages de données entre programmes, qui ne supportent pas toujours bien tant de variations. Mieux vaut disposer de données dites « rangées » lorsqu'on veut effectuer des transferts entre plusieurs programmes : alors, chaque colonne correspond à une variable et chaque ligne correspond à un cas. On se retrouve avec une feuille par fichier et un niveau d'analyse par fichier. C'est une configuration similaire à celle des bases de données relationnelles et qui facilite grandement les transferts de données entre programmes. Que pouvons-nous conclure de cette brève présentation des feuilles de calcul ? Les scientifiques des données en ont besoin. Elles restent des outils essentiels qu'il faut savoir exploiter. Ne serait-ce que parce les clients fournissent des données dans ce format et comptent les récupérer dans ce format. Ce sont aussi des outils de référence, notamment pour le survol des données et les tableaux croisés dynamiques. Cela dit, n'oubliez pas qu'il vous faut des données rangées pour les transferts entre programmes. 


-----------------------------------------------
Vidéo : S'initier au langage R
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:00            S'initier au langage R
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Après les feuilles de calcul, j'aimerais vous parler du rôle de R en data science. Il s'agit d'un langage de programmation statistique, désigné par la lettre R. On peut dire que R est le langage de la science des données. Regardons de nouveau les résultats de cette enquête KDnuggets. Elle concerne l'exploration de données et R est l'outil le plus utilisé dans ce domaine, presque deux fois plus que tous les autres. Il est 50 % plus utilisé que Python, qui est son principal concurrent en matière de packages statistiques spécialisés. Et ce, pour plusieurs raisons. Un, R est open source et donc gratuit. C'est un avantage, car les programmes propriétaires se révèlent parfois extrêmement chers. Deux, R est optimisé pour les opérations vectorielles. De fait, il est capable de traiter toute une série de données sans avoir à écrire explicitement aucune boucle for, ce qui fait gagner du temps. Trois, R s'appuie sur une super communauté. Il existe de nombreuses sources d'assistance et on peut trouver de l'aide pour presque tous les usages de R. Enfin, plus important encore, plus de 7 000 packages gratuits actuellement disponibles étendent considérablement les capacités de R. Maintenant, étudions quelques interfaces R pour voir comment les choses se passent. R a son propre environnement de développement interactif, ou EDI. C'est une interface de ligne de commande classique, mais qui varie d'un système d'exploitation à l'autre et qui dispose de plusieurs fenêtres flottantes, ce qui est un peu gênant. R peut aussi se lancer depuis le terminal ou la ligne de commande. Sur un Mac, il suffit d'ouvrir le terminal et de saisir la lettre R pour l'exécuter. Cela dit, l'interface la plus courante s'appelle RStudio et c'est celle-ci. Il s'agit d'une fenêtre superposée : il faut installer R séparément. Elle organise l'outil et facilite grandement son utilisation. De plus, elle ne varie pas selon le système d'exploitation. Il existe une autre option. L'outil Jupyter est surtout connu des utilisateurs à travers Python et IPython. Il est parfait pour travailler avec du code et partager. En revanche, à mon avis, il n'est pas encore tout à fait prêt pour R, mais je crois volontiers que cela va changer très prochainement. Je rappelle que ces interfaces sont de type ligne de commande et requièrent de saisir des lignes de code, comme ici, en haut à gauche. Certains disent que RStudio est une interface utilisateur graphique, ou IUG. C'est vrai s'il s'agit d'exécuter ou de copier-coller. Mais pour analyser, il faut bien saisir des lignes de code. Maintenant, parlons des commandes qu'on peut trouver dans les lignes de code. Vous pouvez saisir des commandes dans la console. Ou bien les enregistrer pour les exécuter à partir de scripts, et voici justement à quoi cela ressemble. Attention, R diffère un peu des autres langages. Par exemple, vous voyez que les commandes ne se terminent pas par un point-virgule. En revanche, les blancs n'ont pas dans R l'importance qu'ils ont dans Python et, de façon générale, la procédure est un peu inhabituelle. Cela dit, une fois qu'on a pris le pli, R s'avère un outil très logique et redoutablement efficace pour travailler avec des données. Voyons le résultat. Les graphiques s'affichent dans une fenêtre et le texte ou les chiffres, dans une console. Du coup, ils disparaissent ensuite, sauf si vous faites la démarche de les enregistrer. Il est notamment possible d'écrire ces résultats dans des fichiers afin de les conserver. Mais le plus important dans R, c'est l'existence de packages. Il existe une chose du nom de CRAN, sur le site cran.rstudio.com. C'est l'acronyme de Comprehensive R Archive Network. Tous les packages y sont centralisés. On peut y faire une recherche par sujet ou par vue, où les tâches sont présentées par sujet. J'ai ici une partie des packages d'apprentissage des statistiques bayésiennes, dont la liste est très longue : ce n'en est que le début. Dans l'idéal, tout package devrait s'accompagner d'échantillons de jeu de données ainsi que d'un guide de l'utilisateur et d'animations qui démontrent son fonctionnement. Il existe une autre option : Crantastic avec un point d'exclamation. Sur le site crantastic.org. C'est une interface intéressante, car elle est liée au CRAN, mais ses classements par popularité et par ordre chronologique inversé permettent de naviguer facilement parmi les packages proposés. Que pouvons-nous donc conclure de cette brève présentation de R ? Un, R est vital en science des données. Deux, il s'agit d'une interface de ligne de commande, ce qui requiert de saisir des lignes de code. Trois, il est particulièrement attrayant en raison des milliers de packages gratuits disponibles pour étendre ses capacités. 


-----------------------------------------------
Vidéo : Aborder Python
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:02:04            Aborder Python
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
La plupart du temps, lorsqu'on me demande quels outils utiliser en science des données, j'en conseille deux : le langage de programmation statistique R et le langage de programmation généraliste Python. Python est un outil central, indispensable en science des données. Voyons quel rang il occupe dans le résultat de l'enquête KDnuggets. Je rappelle qu'il s'agit des outils les plus utilisés en exploration des données. Python se classe quatrième et c'est le seul langage de programmation généraliste qui est présent dans la liste. De plus, il se généralise. En soi, Python n'est pas nouveau, sauf dans le domaine du travail des données. Or, il s'y étend très rapidement. Et ce, pour plusieurs raisons. Déjà, c'est un outil généraliste qui permet de faire beaucoup de choses et les utilisateurs ne s'en privent pas. On peut créer des applications, accéder à des sources de données et contrôler divers éléments. Ensuite, Python est intégré, préchargé sur les ordinateurs Mac ou Linux. Et facile à installer sous Windows. Comme R, il s'appuie sur une super communauté. Enfin, il compte des milliers de packages. En fait, il y en a des dizaines de milliers, mais seule une fraction concerne la science des données. Maintenant, Python vous impose de prendre une décision au sujet des versions. Il en existe deux, à savoir les versions 2.x et 3.x, toutes deux très répandues. Le problème, c'est que la version 3.x de Python n'est pas entièrement rétrocompatible. Le code écrit pour la version trois peut ne pas marcher avec la deux, et inversement. Par ailleurs, bon nombre de packages reposent encore sur la version deux. Cela est sans doute appelé à changer. Néanmoins, pour le moment, les scientifiques des données utilisent majoritairement la version deux, même si une mise à jour est en cours pour la version trois. Voyons un peu ensemble les interfaces. Lorsque vous travaillez avec Python, vous pouvez choisir son environnement de développement Internet dédié, nommé IDLE. Vous pouvez aussi l'exécuter sur le terminal de la ligne de commande ou dans n'importe quel EDI. Vous pouvez également l'exécuter via l'interface Jupyter, que vous voyez ici, à gauche. À l'origine, elle s'appelait IPython, Jupyter étant le nom du projet global compatible avec plus de 50 langages. Jupyter est actuellement mon interface Python favorite, surtout pour les démonstrations et les partages. L'installation de Jupyter peut s'avérer un peu délicate, à moins de recourir à une sorte d'offre groupée. Je peux vous en conseiller deux. Celle de la société Continuum, qui propose une version baptisée Anaconda, très volumineuse. Elle comprend Jupyter ainsi que des centaines de packages clés et toutes leurs dépendances requises en science des données. Continuum Anaconda est gratuit. Et celle de la société Enthought, qui s'appelle Canopy. Elle existe sous forme gratuite, mais aussi sous forme de produit commercial. Les deux sont vraiment bien et faciliteront grandement vos débuts dans l'univers de Python et Jupyter. Comme dans le cas de R, toutes les interfaces sont de type ligne de commande. Par conséquent, elles requièrent toutes de saisir des lignes de code. Intéressons-nous à certaines de ces lignes de code. Les commandes sont sous forme de texte et voici celles que j'ai saisies pour générer un graphique sur une autre page. L'avantage, c'est que Python est un outil familier de millions de codeurs. Il est très connu et maîtrisé. Il est à la fois simple et clair de travailler dans ce langage. Sans compter qu'il offre bon nombre d'adaptations pour les données. Penchons-nous maintenant sur le résultat. Voici à quoi il ressemble dans Jupyter/IPython, qui est mon interface de prédilection. On obtient du texte, comme vous le voyez dans les cellules grises. On obtient des graphiques insérés, comme il y en avait sur la page précédente. Par ailleurs, le tout est très facile à organiser et à présenter lorsqu'on utilise Jupyter. C'est un de ses grands avantages. Que conclure de tout ça ? Un, Python est un langage extrêmement répandu et familier de millions d'utilisateurs. Deux, il a l'avantage d'être un langage de programmation généraliste, capable de faire ben plus que du travail de données. Trois, comme R, il est attrayant en raison des milliers de packages gratuits mis à disposition par sa communauté en ligne. 


-----------------------------------------------
Vidéo : Découvrir SQL
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:23            Découvrir SQL
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Lorsqu'on fait de la science des données, on ne tarde pas à se rendre compte de l'importance de SQL, ou « sequel ». Voyons quel rang cet outil occupe dans le résultat de l'enquête KDnuggets. SQL se classe troisième parmi les programmes et langages que les professionnels de l'exploration de données utilisent le plus dans leur travail. C'est aussi le premier outil de base de données de la liste. Il faut bien garder à l'esprit que SQL est un langage, et non une application en soi. Il permet de développer des applications. Dans les bases de données relationnelles, c'est l'outil le plus courant. Il convient bien aux données structurées. Autrement dit, aux tables et autres ensemble de lignes et colonnes. En général, on utilise juste SQL pour effectuer et organiser des extractions depuis une base de données, avant de les envoyer pour analyse à un programme comme R ou Python. SQL est le langage des SGBDR, les systèmes de gestion de base de données relationnelle. Ce terme désigne les structures de stockage au sein des bases de données. Les SGBDR les plus courants sont Oracle Database, Microsoft SQL Server, MySQL et PostgreSQL. Ce sont vos principales options et, même si le fonctionnement de SQL peut varier légèrement de l'une à l'autre, il reste similaire et permet de formuler des requêtes ainsi que d'extraire des données afin de travailler avec. Toute base de données dispose d'une structure. Elle vise à réduire le nombre de redondances pour éviter les doublons, autrement dit plusieurs enregistrements de la même information. Ainsi, lorsque vous actualisez une information, elle se met à jour partout. De plus, les données sont organisées en lignes et colonnes. Ces tables sont reliées entre elles, d'où le terme « base de données relationnelle ». Passons aux interfaces : il en existe de type texte, qui sont d'ailleurs nombreuses. Cela dit, beaucoup d'entreprises conçoivent des interfaces utilisateur graphiques, ou IUG, compatibles avec SQL et les bases relationnelles. Par exemple, nous pouvons citer SQL Server Management Studio et SQL Developer. Toad est également assez courant. En réalité, toute interface de ligne de commande ou tout environnement de développement interactif permet d'écrire du code dans le langage SQL et d'accéder aux bases relationnelles. Intéressons-nous à quelques commandes SQL de base. La première, que vous n'allez pas cesser d'utiliser, est SELECT. C'est la commande à saisir pour effectuer une extraction depuis une base de données. FROM est la commande qui permet de préciser dans quelle table prélever les données. WHERE permet d'indiquer des critères : où l'âge est supérieur à 27 ans, où le genre est féminin, etc. Enfin, ORDER BY permet d'opérer un tri. Naturellement, il existe bien d'autres commandes SQL, mais celles-ci sont les plus fréquemment utilisées. Elles permettent d'obtenir des données, puis de vite les nettoyer et les organiser afin de les exploiter dans d'autres langages. Voici ce qu'on peut en conclure. Un, SQL est le langage de l'extraction à partir des bases relationnelles. Deux, une poignée de commandes suffit. Trois, les données sont souvent extraites, puis envoyées pour analyse à d'autres programmes comme R ou Python. 


-----------------------------------------------
Vidéo : Explorer les formats web
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:47            Explorer les formats web
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Dans le cadre d'un projet de science des données, vous pouvez trouver des données dans des feuilles de calcul, dans des bases relationnelles, mais aussi sur Internet. C'est pourquoi, vous devez connaître un peu ce que j'appelle les formats web. En effet, la science des données s'épanouit sur Internet, aussi bien en matière de collecte qu'en matière de partage. Voici quelques points à connaître. D'abord, HTML, ou HyperText Markup Language. Ensuite, XML, sigle d'Extensible Markup Language. Ensuite, JSON, ou JavaScript Object Notation. Enfin, JavaScript, qui est, lui, plutôt un langage de programmation. Nous allons les étudier tour à tour. HTML, ou HyperText Markup Language, est le langage des pages web ; il permet de délimiter le texte ou les titres et d'indiquer l'emplacement des liens. Les pages web sont basées sur des feuilles de style CSS, acronyme de Cascading Style Sheets. C'est ce qui définit la présentation et l'aspect du contenu sur les pages web. Il faut savoir naviguer dans HTML pour faire du scraping ou des extractions de données sur Internet. C'est sans doute l'avantage majeur du format HTML pour les scientifiques des données. Pour ce qui est de la structure des données, on rencontre très couramment XML, ou Extensible Markup Language. Cette méthode de codage s'avère lisible à la fois par l'homme et par la machine, ce qui n'est pas toujours le cas. C'est un format semistructuré. Autrement dit, les balises sont librement configurables, peuvent faire l'objet de recherches et donnent à l'ordinateur l'emplacement des données. J'ai ici la balise RaceTable entre les signes inférieur et supérieur. Il s'agit donc d'une table d'informations sur les courses de F1. Ensuite viennent la saison, le Grand Prix, le circuit, etc. Un autre format extrêmement répandu est JSON, ou JavaScript Object Notation. À l'origine, il est dérivé du langage JavaScript, mais il est totalement indépendant aujourd'hui. Il marche avec de nombreux autres langages. JSON se substitue progressivement à XML, notamment parce qu'il se révèle un peu plus compact. Cela dit, les traductions sont faciles de l'un vers l'autre. Ici, j'ai repris mon exemple en XML et je l'ai traduit en JSON, tout simplement. Les accolades, les points-virgules et les guillemets sont des moyens de définir les balises ainsi que les données. Enfin, JavaScript. Avec HTML et CSS, JavaScript compte parmi les grands langages présents sur Internet. Il s'agit d'un langage de programmation autonome qui permet de créer des applications de A à Z. Il sert aussi à créer des éléments interactifs dynamiques, comme les graphiques de base de données. Le meilleur exemple de cet usage est sans doute le langage D3, abréviation de Data-Driven Documents. D3.js sert à créer des visualisations de données interactives à intégrer aux pages web. La bibliothèque JavaScript D3 se fonde sur des données pour générer des graphiques vectoriels évolutifs, ce qui est absolument génial. Selon moi, la possibilité d'utiliser D3 est la raison première pour laquelle les scientifiques des données devraient se former à JavaScript. Voici ce que nous pouvons en conclure. Un, les données web sont vitales en science des données, car Internet regorge de sources contenant d'énormes volumes d'informations. Deux, ces sources utilisent les formats HTML, XML ou JSON. Trois, D3.js est une bibliothèque JavaScript permettant de créer des visualisations dynamiques à partager sur Internet. 


***********************************************
Chapitre : 7. Connaître les mathématiques
***********************************************


-----------------------------------------------
Vidéo : Faire un point sur l'algèbre
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:54            Faire un point sur l'algèbre
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
L'algèbre s'avère essentielle en science des données. C'est l'étude des quantités abstraites et des relations qui existent entre elles. Il y a trois types d'algèbre particulièrement pertinents en science des données. Un, l'algèbre élémentaire, celle que nous apprenons tous à l'école pour calculer des valeurs avec les multiplications, les additions, etc. Deux, l'algèbre linéaire, dite « algèbre matricielle ». Celle qui est au cœur des calculs des procédures statistiques. Enfin, trois, les systèmes d'équations linéaires. Ils sont essentiels à l'algèbre linéaire ainsi qu'à la pratique de l'optimisation, dont allons reparler. Maintenant, voyons un exemple concret basé sur les salaires des scientifiques des données. Je pose l'équation salaire égale une certaine constante plus des années – qui vont avoir un lien avec l'âge – plus la négociation plus des heures plus une erreur. On pourrait l'écrire ainsi, avec des initiales, mais il est plus courant de l'écrire comme ceci. Laissez-moi vous expliquer ces termes. Tout à gauche, y indice i représente une variable de résultat pour la personne i (i valant un, deux, trois, etc.). Ensuite, bêta indice zéro – on est dans les lettres grecques – correspond au point d'intersection y. Puis, bêta indice un représente le coefficient de régression de la variable un. Après, nous avons x indice un i. C'est le score de la variable un pour la personne i. Tout à droite, epsilon indice i correspond à l'erreur de prévision pour la personne i. En matière d'algèbre, il est important de savoir qu'il existe différentes structures possibles. Le type scalaire est le plus habituel, avec seul un chiffre à la fois, mais il existe le type vectoriel, avec une ligne ou une colonne de chiffres traités comme une même unité, ainsi que le type matriciel, avec plusieurs lignes et colonnes de données groupées dans un objet. Ces éléments permettent de réécrire l'équation comme ceci. Cette formule combine vecteurs et matrices. Ce vecteur, à gauche, représente les résultats des cas un et deux. Cette matrice, au milieu, comprend toutes les données des personnes. La ligne du haut contient un 1 multiplié par une constante plus les scores des trois variables pour le cas un. Le cas deux est sur la ligne du bas. Cette colonne comprend les coefficients de régression, à savoir bêta indice zéro, bêta indice un, etc. Enfin, ce plus petit vecteur représente les erreurs pour les cas un et deux. Voyons comment ça marche avec un exemple fictif. Prenons deux scientifiques des données : Fatima, 28 ans, assez forte en négociation avec un score de quatre sur cinq, qui travaille 50 heures par semaine pour 118 000 $ par an et Ezra, 34 ans, moyennement solide en négociation, qui travaille heures par semaine pour 84 000 $ par an. Ce que nous allons faire maintenant, c'est revenir à l'équation pour calculer le salaire de Fatima. Voici la formule que nous utilisons. Mettons les chiffres de l'exemple. Fatima gagne 118 000 $ par an. On multiplie alors chacune des données correspondant au cas de Fatima par ces coefficients, en prenant les données de gauche à droite et les coefficients de haut en bas. On multiplie donc ces deux chiffres, puis ces deux-là, puis ces deux-là et ces deux-là, on les additionne puis on ajoute une erreur. Vous voyez que le terme d'erreur est énorme dans le cas de Fatima. C'est parce que, même si j'utilise ici des données réelles, l'équation comptait 35 autres variables, notamment celle selon laquelle, si Fatima avait été PDG, elle aurait dû gagner 30 000 $ de plus par an. Ce qui est génial, c'est de le traduire en notation matricielle. Cela permet de synthétiser toute cette série de matrices et de vecteurs sous cette forme, où le y en gras représente le vecteur des variables de résultat, le x en gras représente la matrice des données des personnes, le bêta représente le vecteur des coefficients de régression et l'epsilon représente le vecteur des erreurs. Cette formule extrêmement compacte se retrouve dans la plupart des programmes statistiques. Voici à quoi cela ressemble dans R. J'ai ouvert R et je vais reprendre les mêmes données pour cette démonstration. Je vais créer un vecteur comprenant les résultats, autrement dit les salaires. Je précise qu'on utilise des minuscules pour les vecteurs et des majuscules pour les matrices. Donc, je crée ce vecteur et je vais vous le montrer. Le voilà, il s'affiche à l'écran. Il faut cbind pour en faire une colonne. Ensuite, je créer une matrice, qui va s'afficher… et voilà exactement ce qu'on voulait. Je crée un vecteur b pour les coefficients de régression, avec cbind pour en faire une colonne. Le fait que les données soient arrangées en colonne ou en ligne change tout en algèbre matricielle, comme l'ordre des opérations, d'ailleurs. A fois B n'est pas pareil que B fois A lorsqu'on utilise des matrices. Maintenant, je calcule la valeur prévue de y pour chaque personne, autrement dit le résultat, en multipliant la matrice des données par le vecteur des coefficients de régression. Vous voyez alors apparaître ici les deux scores prévus. Pour calculer le terme d'erreur, il suffit de soustraire ces scores Si on compare ces chiffres à ceux obtenus précédemment, on voit qu'ils correspondent. Il est très facile d'effectuer ces calculs dans R. Que peut-on conclure de tout ça ? Un, l'algèbre s'avère vitale. C'est la clé de la science des données. Deux, les matrices simplifient la notation. Trois, l'algèbre linéaire se trouve au cœur de nombreuses procédures en science des données. procédures en science des données. 


-----------------------------------------------
Vidéo : Découvrir les systèmes d'équations
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:48            Découvrir les systèmes d'équations
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
La capacité à travailler avec des systèmes d'équations linéaires compte beaucoup en science des données. La question est : comment s'en sortir avec plusieurs inconnues ? Le problème de l'interdépendance semble redoutable, par exemple lorsque X dépend de Y, mais que Y dépend de X. Cela paraît particulièrement complexe, et pourtant les systèmes d'équations linéaires peuvent se résoudre à la main. On peut aussi faire de l'algèbre linéaire ou matricielle, comme nous le verrons. Imaginons que vous fabriquiez et vendiez des boîtiers pour smartphone. Mettons que vous ayez vendu 1 000 boîtiers en tout, certains à 20 euros, d'autres à 5 euros, pour un total de 5 900 euros. Combien de chaque en avez-vous vendu ? Traduisons l'énoncé du problème en équations. On a celle des ventes : x, le premier prix, plus y, le second prix égalent un total de 1 000 boîtiers. Et on a celle du chiffre : 5 900 euros est égal à x fois 20 euros plus y fois 5 euros. Il ne nous reste plus qu'à trouver un moyen de fondre ces deux équations en une pour trouver la solution. Regardons d'abord les ventes. On a x + y, donc les ventes au premier prix et les ventes au second prix, égalent 1 000. Il y a aussi le chiffre. Mais restons d'abord sur les ventes. Nous allons calculer la valeur x en faisant, pour cela, passer le y de l'autre côté. Il suffit de soustraire y des deux côtés pour le faire disparaître à gauche et obtenir une valeur x exprimée uniquement en y. On peut alors reprendre l'équation afin de remplacer le x par cette nouvelle formule. On fait les multiplications, puis on additionne -20y et 5y, ce qui donne -15y et permet de calculer la valeur y. Pour cela, on soustrait juste 20 000 de chaque côté. On commence par l'enlever d'un côté, puis on l'enlève de l'autre côté ; enfin, on divise les deux par - Si on reprend l'équation des ventes, x + y égalent 1 000, on remplace y par 940, puis on le soustrait des deux côtés, ce qui donne x égale 60. Cela signifie que vous avez vendu 60 boîtiers au prix unitaire de 20 euros et 940 boîtiers, nettement plus, au prix unitaire de 5 euros. Il est même possible de faire un graphique. Le problème, c'est que les équations groupent initialement x et y à gauche, face à une constante à droite. Dans les deux cas, il faut calculer la valeur y en soustrayant x des deux côtés pour résoudre l'équation du haut. Ensuite, on divise tout par puis on se débarrasse de 4x en soustrayant cet élément des deux côtés, et voilà : dans les deux équations, y est une fonction de x. En représentation graphique, la ligne rouge tracée ici montre le nombre d'unités vendues. Au départ, on avait x + y = 1 000, ce qui équivaut à y = -x + 1 000, et cette ligne illustre toutes les combinaisons de ventes possibles. La ligne bleue, elle, représente le chiffre, soit 20x + 5y = 5 900. Cela s'exprime aussi en y pour illustrer toutes les combinaisons de chiffre possibles. Vous voyez que nous avons ici un point d'intersection, qui donne la solution combinée des deux équations, à savoir 60 et 940. On peut aussi le faire dans R. Ce que je vais vous montrer maintenant, c'est comment saisir ces données. Voici nos équations : x + y = 1 000 et 20x +5 y =5 900. Je vais prendre les coefficients, qui se trouvent à gauche, et les saisir dans une matrice. Je l'appelle Q, abréviation de quantité, et j'obtiens cette matrice, qui permet de saisir ensuite les résultats dans un vecteur. Je vais les mettre ici, puis je vais utiliser la fonction de résolution intégrée, qui se récupère à l'aide de cette simple commande. Regardez, dans ce volet, R résout le système d'équations. Il nous reste à résoudre Q pour le chiffre : vous voyez que nous obtenons les valeurs 60 et 940, qui correspondent à celles que nous avons calculées à la main précédemment. Si vous voulez contrôler les calculs, vous pouvez venir ici pour exécuter une fonction CES et voir qu'on obtient bien les sommes de 1 000 et de 5 900. Que pouvons-nous conclure de cela ? Un, les systèmes d'équations linéaires ont une importance capitale en science des données. Deux, ils sont vitaux pour jongler avec plusieurs inconnues afin de trouver une solution. Trois, ils reposent sur l'algèbre linéaire ou matricielle, même s'il est possible de les calculer à la main, ce qui reste rare en science des données. 


-----------------------------------------------
Vidéo : Effectuer un calcul infinitésimal
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:02:51            Effectuer un calcul infinitésimal
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Le calcul infinitésimal est un fondement incontournable de la data science. Par exemple, tous les rythmes de changement et toutes les optimisations se déterminent à l'aide d'un calcul infinitésimal. Il intervient dans trois domaines précis de data science. Un, le calcul infinitésimal étant un fondement statistique de la science des données, il sert pour des choses telles que la régression des moindres carrés et la distribution des probabilités. Deux, on le retrouve également dans la mesure des quantités ou des taux qui évoluent dans le temps. Trois, on l'utilise en général pour calculer les maxima et les minima, à chaque fois qu'on veut maximiser ou minimiser quelque chose. Sachez qu'il existe deux types de calcul infinitésimal, qui nous intéressent tous les deux. D'abord, le calcul différentiel, pour les rythmes de changement à un moment M. Certains l'appellent donc le calcul du changement. Ensuite, le calcul intégral, pour mesurer la quantité d'un élément précis à un moment M selon un rythme de changement donné. Certains l'appellent donc le calcul de l'accumulation. Découvrons ensemble son fonctionnement. Nous avons ici une parabole ; y égale x au carré. Nous pouvons mettre un point ici, où x égale moins deux. Pour trouver la valeur y, il suffit de passer par le calcul de x au carré, ce qui nous donne quatre. Mais quelle est la pente pour x égale moins deux ? En effet, sur une courbe, la pente varie en permanence. Pour effectuer ce calcul, il faut utiliser une dérivée, qui est ici d'une nature toute simple. Voici la formule correspondante, où x puissance n est x au carré dans ce cas précis. Remplaçons donc les n par des deux pour obtenir la dérivée. On le fait ici, puis on le fait ici aussi. Pour finir, ici, et deux moins un égalent un. Le chiffre un disparaît. La dérivée de x au carré est donc 2x. Cela nous donne la pente correspondant à ce point. Il suffit d'effectuer le calcul en remplaçant x par moins deux, ce qui nous donne une pente de moins quatre, représentée par cette ligne rouge. Si nous déplaçons le point là où x égale trois, nous pouvons calculer la pente de la même façon : 2x égale six, et voici sa ligne. Pour prendre un exemple concret, imaginons que vous ayez conçu un service de rencontre en ligne et que vous vouliez trouvez le prix idéal pour maximiser votre chiffre d'affaires. Place au calcul infinitésimal. Voici ce que nous allons faire : nous allons dire que vos abonnements annuels sont actuellement au prix de 500 euros. Certains sites de rencontre pratiquent ce type de tarif. Vous enregistrez 180 ventes ou nouveaux abonnements par semaine. De plus, certaines données semblent indiquer que chaque remise de euros sur le prix se traduit par trois ventes supplémentaires. Comme vous proposez un service en ligne, dépourvu de processus physique, nous allons dire que les frais généraux sont dérisoires. Quel prix permettrait de maximiser votre chiffre d'affaires ? Nous allons exprimer le prix sous cette forme : 500 euros, le tarif actuel, moins 5 euros fois r, qui représente les unités de remise. Le r permet d'uniformiser les unités du prix et des ventes. Ensuite, les ventes égalent 180 nouveaux abonnements par semaine plus 3 pour chaque unité de remise r. Voyons les ventes comme une fonction du prix. Pour cela, il faut réécrire les équations avec un point d'intersection et une pente. Commençons par l'intersection y. Il faut savoir à quelles ventes s'attendre lorsque le prix est fixé à 0 euro. Naturellement, vous ne pratiqueriez pas ce tarif, mais nous en avons besoin pour nos calculs. Donc, combien d'unités de remise faut-il pour arriver à un prix de 0 euro ? Résolvons l'équation du prix dans ce sens. Déjà, on soustrait 500 de chaque côté. Cela s'annule, puis on divise chaque côté par moins euros afin d'isoler le r. Cela donne la valeur 100 pour r, soit 100 tranches de remise de 5 euros pour arriver à un prix de 0 euro. C'est ainsi que nous trouverons l'intersection y. Pour cela, nous allons substituer cette valeur au r dans l'équation des ventes. On prend cette valeur r (les unités de remise) et on l'insère dans l'équation des ventes afin de trouver le résultat pour un prix de 0 euro. On remplace donc le r par 100, on multiplie et on additionne. Voici le point d'intersection y, à savoir les ventes pour un prix nul. Encore une fois, vous ne pratiqueriez pas ce tarif : il permet simplement de trouver la pente. Justement, la pente est égale au changement de y (l'axe vertical) pour tout changement de x (l'axe horizontal). Les ventes sont le y, ou résultat ; le prix est le x, ou prédicteur. Les coefficients qu'ils utilisent pour les remises permettent de trouver la pente. On divise le trois des ventes par le moins cinq du prix pour obtenir une pente de moins zéro virgule six. Ensuite, on combine ce résultat au point d'intersection de 480. Et on obtient l'équation des ventes comme fonction du prix. Elle nous permet de poser l'équation du chiffre d'affaires, ce qui est le but de cet exercice. Chiffre est égal à ventes fois prix. On peut insérer l'équation des ventes que nous venons de trouver afin d'obtenir une équation de chiffre exprimée uniquement en prix. On multiplie et on obtient ce résultat. Il est alors possible de trouver la dérivée, car on cherche ici à maximiser. C'est la partie infinitésimale du calcul. La dérivée de 480 fois le prix est juste 480. Du coup, le prix disparaît. La dérivée du second élément fonctionne comme dans notre exemple précédent pour trouver la pente d'un point sur une courbe. On déplace le deux ici, on multiplie et voici la dérivée. Il reste à calculer pour un résultat nul. Je vous explique pourquoi. L'équation du chiffre d'affaires donne une parabole inversée. Le y, à savoir le chiffre d'affaires, est au maximum lorsque la pente de la courbe est nulle, car le zéro est son point le plus haut. Cette ligne rouge illustre la pente nulle, qui est donc toute plate. Nous voulons trouver l'emplacement de ce point de la courbe qui croise la ligne plate là où la courbe est à zéro. Revenons à l'équation, que nous allons donc résoudre pour un prix nul. On soustrait 480 partout et on divise par -1, afin d'isoler le prix. Cela donne 400 euros pour le point permettant de maximiser le chiffre d'affaires. On peut calculer les ventes correspondant à ce point en résolvant l'équation qui exprime les ventes en termes de prix. On insère la valeur 400, on multiplie et on soustrait pour obtenir 240 ventes de nouveaux abonnements par semaine. Voyons quel effet cela aurait sur le chiffre. Voici la formule de votre modèle actuel. Le chiffre est ici égal à 180 fois 500. Cela donne un chiffre de Comparons-le avec le nouveau modèle et ses ventes attendues de 96 000 euros. Pour calculer l'amélioration, on divise les deux et 1, signifie que le chiffre devrait augmenter de 7 % grâce au nouveau modèle. En bref, vous baissez de en passant de 500 euros à 400 euros. Cela devrait augmenter de 33 % vos ventes, qui passeraient de Ensemble, ces changements doperaient votre chiffre à hauteur de 7 %. J'ai effectué tous ces calculs à la main, et c'est tout à fait faisable, bien sûr, mais j'ai le plaisir de confirmer que vous pouvez aussi les effectuer sur ordinateur. Nous allons voir comment procéder dans R. J'ouvre R et je commence par écrire la formule qui correspond aux ventes comme fonction du prix. Je l'enregistre même dans cet objet, nommé sales. Puis, j'écris la formule du chiffre d'affaires comme fonction du prix et des ventes. Ensuite, je crée deux graphiques côte à côte, par le biais de cette manipulation à l'aide de l'objet par pour paramètre. D'abord, celui des ventes comme fonction du prix. Voici ce premier graphique, que je vais agrandir. Comme vous le voyez, quand le prix monte, les ventes baissent. Et quand le prix baisse, les ventes montent. Passons à celui du chiffre comme fonction du prix et des ventes. Il vient s'afficher à côté. On retrouve la parabole inversée qu'on a vue précédemment. Traçons la ligne du prix qui procure le chiffre maximum. Elle se situe au niveau de 400 euros. Traçons la ligne correspondant à la pente nulle sur la courbe du chiffre. Elle se situe à 96 000 euros. Vous la voyez ici, tout en haut. Mettons un point cette intersection. Il apparaît juste ici. Il faut restaurer les paramètres des graphiques, comme à chaque fois qu'on a fini de les remanier. Ensuite, je peux effectuer en bloc tous les calculs infinitésimaux grâce à la fonction intégrée optimize. Je précise que je veux optimiser le chiffre d'affaires, en cherchant dans la tranche de 100 à 700 euros, et qu'il me faut un maximum. Vous voyez les résultats de cette fonction s'afficher juste ici. 400 euros est le prix de vente qui maximise le chiffre à 96 000 euros. On obtient bien les mêmes résultats qu'avec les calculs à la main de la dérivée pour trouver la solution optimale. Quelles conclusions pouvons-nous tirer de cette présentation ? Un, le calcul infinitésimal est vital en science des données. Deux, c'est le fondement des analyses statistiques. Trois, il sert à résoudre les problèmes d'optimisation. 


-----------------------------------------------
Vidéo : Utiliser le grand O
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:01:42            Utiliser le grand O
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Un sujet important en science des données si on veut éviter les ennuis, c'est le grand O et l'exécution des fonctions. C'est vital lorsque le temps est un facteur clé. En effet, les fonctions appliquées aux données varient sur le plan de la vitesse. Le taux de croissance d'une fonction, donc le temps qu'elle prend, s'appelle l'ordre. D'où le nom de grand O, signifiant grand ordre. La notation grand O décrit les taux de croissance, dont les différences peuvent se révéler radicales et parfois contre-intuitives. Par exemple, certaines fonctions sont très rapides. Nous avons O(1), constante déterminante pour les nombres binaires, pairs ou impairs. Avec O(log(n)), de type logarithmique. Et O(n), de type linéaire. Elles sont toutes plutôt rapides. O(n log(n)) est plutôt moyenne. On l'utiliserait dans le cadre d'une transformée de Fourier rapide pour accélérer sensiblement le processus de la transformée discrète. Du côté des fonctions lentes, et je veux dire extraordinairement lentes, on a O de n au carré, de type quadratique. Mais aussi O de deux puissance n, de type exponentiel. Et O de n factoriel, qui est l'approche la plus lente qui soit imaginable. Je vais vous montrer comment cela marche. Certaines fonctions sont bien. Le tri par insertion permet de trier les entrées de la liste en place. Dans le meilleur des cas, le tri par insertion est très rapide ; il a un ordre n, c'est bien. Cela dit, en moyenne, le tri par insertion est très lent et varie du linéaire, comme ici, au quadratique. En revanche, le processus très similaire du tri par sélection est lent dans le meilleur des cas et en moyenne. On a donc un degré de variabilité élevé parmi les tris par insertion et faible parmi les tris par sélection. Or, on ne pense pas qu'ils puissent être différents, et c'est pourquoi vous devez savoir à quoi vous attendre lorsque vous choisissez les fonctions que vous souhaitez utiliser. Je vais vous montrer un peu comment cela marche dans R. Nous allons créer un graphique illustrant la vitesse de ces diverses fonctions. Nous allons prendre le package RColorBrewer pour ajouter des couleurs. C'est un outil génial pour créer des graphiques. Il suffit de le charger. Maintenant, paramétrons notre graphique. On indique les limites de la courbe pour l'axe X et l'axe Y. Puis, on sélectionne une palette spécifique de RColorBrewer. Et place au graphique des fonctions. D'abord, la plus rapide : la constante. Pour cela, nous allons définir la courbe à l'aide des divers paramètres que nous avons ici. Comme vous le voyez, ici en plus grand, on a une constante. Indépendamment du nombre d'éléments dont on dispose, la fonction a toujours la même durée. Une fonction logarithmique prend plus de temps. Regardons cela. Comme vous le voyez, elle monte, mais très légèrement. Une fonction linéaire prend plus de temps. Cette augmentation peut sembler considérable, mais elle n'est rien comparée à celles que nous allons voir. Une fonction moyenne – comme, ici, Loglinear – est plus lente, mais géniale dans le cadre d'une transformée de Fourier rapide. La courbe monte bien plus haut dans ce cas : si vous effectuez vos calculs avec seulement 10 éléments, le processus se révèlera environ 23 fois plus long que celui d'une constante. Place aux fonctions lentes, avec la quadratique. Comme vous le voyez, la courbe correspondante monte en flèche. Puis, l'exponentielle. Si vous connaissez le problème du voyageur de commerce, c'est sa résolution la plus rapide via la programmation dynamique. Elle monte haut. Mais la pire de toutes, c'est la factorielle. Elle consiste à résoudre le problème dit du voyageur de commerce à l'aide d'une recherche par force brute qui conduit à passer en revue toutes les combinaisons possibles. Le temps que cela prend est représenté par cette courbe, qui est presque une ligne verticale. Je vais agrandir de nouveau… mais attendez, je vais d'abord ajouter une légende. Je trace également une ligne de repère au niveau de 10 éléments. Elle s'affiche ici en pointillé. Avec 10 éléments, la fonction constante avance unité par unité. La fonction logarithmique prend un peu plus de temps, mais la fonction factorielle s'avère trois millions et demi de fois plus lente. Alors, à moins d'avoir une patience d'ange, mieux vaut savoir si l'algorithme ou la fonction qu'on souhaite utiliser se révèlera efficace en matière d'emploi des ressources temporelles et informatiques. Voici ce que nous pouvons en conclure : les fonctions varient sensiblement sur le plan de la vitesse, mais aussi de façon inattendue par tâche et il faut toujours être conscient des exigences informatiques relatives aussi bien à la taille du jeu de données à traiter qu'aux tâches spécifiques à exécuter. 


-----------------------------------------------
Vidéo : Appliquer le théorème de Bayes
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:02:20            Appliquer le théorème de Bayes
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Le théorème de Bayes est un outil important qui permet de voir l'autre côté des choses dans une analyse de données. Plus précisément, il permet de répondre à la bonne question. La plupart des tests inférentiels donnent la probabilité des données (l'effet observé) en fonction d'une hypothèse (la cause supposée). Or, c'est l'inverse que nous voulons, en général. Nous voulons la probabilité de l'hypothèse en fonction des données observées, ce qui peut donner une réponse radicalement différente. Il faut savoir réconcilier les données disponibles. Heureusement, le théorème de Bayes nous aide. Thomas Bayes était un pasteur et statisticien anglais du XVIIIe siècle. Faute de savoir à quoi il ressemblait, nous l'avons représenté ici par une simple silhouette. Son théorème se fonde sur les probabilités antérieures et des tests pour calculer les probabilités postérieures. Il s'agit donc des probabilités avant et après la collecte des données. D'abord, la probabilité des données en fonction de l'hypothèse. Cette notion porte également le nom de « sensibilité ». C'est le résultat normal d'une vérification d'hypothèse. À cela s'ajoute la probabilité de l'hypothèse, donc de la cause. C'est la probabilité antérieure, autrement dit la fréquence de base de la situation en question. Puis s'ajoute la probabilité des données. C'est l'éventualité d'obtenir ce type de résultat particulier. On l'appelle « marginale ». La combinaison du tout donne la probabilité de l'hypothèse en fonction des données observées, autrement dit l'éventualité après le fait, dite « probabilité postérieure ». Cette combinaison se formule de cette façon. La postérieure est la probabilité fois l'antérieure, le tout divisé par la marginale. Voici les symboles correspondants. C'est plus facile à comprendre avec un graphique, alors nous allons voir ensemble un exemple. Imaginons que ce carré représente une population tout entière. Disons qu'une maladie se déclare et que ce rectangle plus foncé, en haut, représente les personnes malades. Nous réalisons un test de dépistage, qui se révèle capable d'identifier 90 % des personnes souffrant de la maladie. Ce qui signifie que le résultat est un faux négatif pour 10 % des malades. Donc, ce test identifie 90 % des personnes souffrant de la maladie, ce qui est très bien. Cela soulève une question. Si le résultat est positif pour une personne, quelle est la probabilité qu'elle soit effectivement malade ? Je vous donne un indice : ce n'est pas 90 %. Ce taux de 90 % concerne les personnes qui souffrent effectivement de la maladie. Or, il faut tenir compte du fait que le test peut donner des faux positifs. Regardons la population saine et prévoyons que pour un certain pourcentage, représenté par cette barre bleue, le résultat du test sera malgré tout positif. Donc 90 % de la population saine obtiennent un résultat négatif, tandis que 10 % obtiennent, à tort, un résultat positif. Il y a certains éléments à connaître afin de calculer la probabilité d'être malade si le résultat du test est positif. Il faut savoir combien de malades ont un résultat positif et diviser ce nombre par le total des personnes dont le test est positif, même s'il s'agit d'un faux positif. Nous allons donc prendre ici le chiffre de 29, %. Puis, ce chiffre de 6,7 %. Ensuite, on les combine. Addition, puis division. On a 81,6 %. Si votre test est positif, vous risquez donc à 81, % de souffrir effectivement de la maladie. Notez que c'est moins que les 90 % qui pouvaient nous trotter dans la tête. Pas beaucoup moins, certes, mais que se passe-t-il si les valeurs changent ? Imaginons une maladie plus rare, qui touche non pas 33 % de la population, mais un pourcentage nettement moins élevé. Peut-être 5 % seulement. En conservant la même sensibilité, vous voyez que le total des personnes malades dont le test est positif ne représente plus que 4, % de la population. Les faux positifs concernent maintenant 9, % de la population. Pour connaître la probabilité d'être effectivement malade si le test est positif, il suffit de prendre ces chiffres et de faire le calcul. On a 32,1 %. Il existe donc moins d'un tiers de risques que vous soyez malade si votre test est positif. Cela peut s'illustrer un peu plus facilement en prenant un exemple dans R. Je vais refaire ici quelques calculs que nous venons de voir ensemble. Je crée la variable correspondant à la probabilité d'être malade. Elle s'appelle « pd » pour probability of disease et je l'associe à 1 % de la population. Puis je définis la probabilité d'un test positif en cas de maladie sur 99, d'une grande sensibilité. Enfin, je mets la probabilité d'un test positif en cas d'absence de maladie à 10 %. C'est le taux de faux positifs. Je peux maintenant calculer la probabilité de maladie en cas de test positif à l'aide du théorème de Bayes. J'exécute la formule et elle me donne 9 %. La sensibilité de 99, des faux positifs ni l'effet de la fréquence de base de la maladie dans la population. En combinant ces deux facteurs, si votre test est positif, la probabilité que vous soyez effectivement malade reste très faible. Je vais vous le montrer sous forme graphique avec une courbe de probabilité. Je vais créer ici un graphique qui montre les probabilités d'être malade, sur une échelle de à 100 en abscisse. Attendez, je vais agrandir le résultat. Nous avons la probabilité antérieure, donc la fréquence de la maladie, graduée de 0 à gauche à 1, autrement dit 100 %, à droite. Ensuite, en cas de test positif, quelle est la probabilité que vous soyez effectivement malade ? C'est la probabilité postérieure, en ordonnée. Je vais ajouter quelques lignes de repère pour un peu plus de clarté. Je reviens ici. J'exécute cette ligne et cette ligne. Vous voyez que cela donne une ligne verticale qui représente le 1 % de la population qui souffre de la maladie. Elle apparaît tout à gauche. La ligne horizontale représente les 9 % de risques d'être malade si le test est positif, en fonction de l'hypothèse selon laquelle seul 1 % de la population est touché. Je peux apporter un complément d'information en préparant toute une série de graphiques pour divers degrés de sensibilité et divers taux de faux positifs. Pour cela, je crée une matrice avec un code qui laisse un peu à désirer, mais qui fait tout de même l'affaire ici. Je commence par cette courbe. Tout y est similaire. Je crée ce graphique d'abord. Il s'affiche en petit à côté, en haut à gauche. Ensuite, je crée plusieurs autres graphiques. J'agrandis le résultat, et voilà. Nous avons ici trois tests avec une sensibilité de 99, % sur la ligne du haut, qui tombe à 95 % au milieu et enfin 80 % en bas. Nous avons un taux de faux positifs de 1 % à gauche, qui passe à 10 % au centre et enfin à 25 % à droite. Le premier graphique, en haut à gauche, signifie que si le test est très sensible et la maladie, très rare, avec un taux très faible de faux positifs, il est presque certain qu'un faux positif est en réalité un vrai positif. En revanche, le dernier graphique, en bas à droite signifie que si le test n'est pas très sensible, si les taux de faux négatifs comme de faux positifs sont élevés et si la maladie n'est pas très courante, un résultat positif peut ne rien vouloir dire, en raison de l'effet du taux de fréquence de base. Voilà la contribution du théorème de Bayes à la bonne interprétation des résultats obtenus. Il s'agit de poser la probabilité antérieure afin de calculer la probabilité postérieure. Voici ce qu'on peut conclure de tout ça. La détention des informations relatives aux probabilités antérieures est un prérequis à ces calculs. En l'absence de ces données, une courbe de probabilité permet d'obtenir une gamme de valeurs possibles. Enfin et surtout, le théorème de Bayes ainsi que les calculs associés représentent votre meilleure chance de trouver la bonne réponse à la question que vous ou votre client souhaitez étudier. 


***********************************************
Chapitre : 8. Aller plus loin avec les statistiques appliquées
***********************************************


-----------------------------------------------
Vidéo : Vérifier une hypothèse
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:40            Vérifier une hypothèse
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
La vérification d'hypothèse est l'une des approches les plus courantes qui soient en statistiques inférentielles. Elle consiste à tester directement la théorie qui est avancée. Elle se compose de deux grandes étapes. On commence par calculer la probabilité que X – autrement dit le résultat observé – arrive par hasard, si son caractère aléatoire était la seule explication possible. Puis, si cette probabilité est faible, on peut rejeter le hasard comme explication probable du résultat observé. Voilà les bases de la vérification d'hypothèse. Cette approche se révèle particulièrement utile dans certains cas. Par exemple, elle est très courante chez les chercheurs scientifiques qui souhaitent savoir si leur théorie est valide ou non. Elle se rencontre aussi dans les diagnostics, quand on essaie de cerner la probabilité d'un résultat précis en fonction d'un test. C'est également le principe fondateur des décisions de sélection, qui consistent à savoir si on remplit une condition sine qua non ou un critère précis. La vérification d'hypothèse nulle – c'est le nom complet de cette opération – implique deux hypothèses ou théories à tester. Pour commencer, l'hypothèse nulle. Elle se note grand H indice zéro. Elle signifie qu'il n'existe pas d'effet systématique, donc aucune différence constante entre les moyennes de groupe ni association entre les variables, et que l'erreur d'échantillonnage aléatoire est la seule explication des observations. Ensuite vient l'hypothèse alternative, qui peut se noter grand H indice A ou encore grand H indice un. Elle signifie qu'il existe bien un effet systématique, avec une différence constante entre les moyennes de groupe et une association entre les variables. L'essentiel à retenir, c'est le nom de vérification d'hypothèse nulle. Il s'agit donc d'étudier l'hypothèse nulle selon laquelle il n'existe pas d'effet systématique. La voici sous forme graphique, avec une distribution nulle. Ici, il s'agit du test z. Vous voyez les résultats possibles si l'hypothèse nulle est vraie, donc si seule l'erreur d'échantillonnage aléatoire explique la différence entre les moyennes de groupe. La plupart des scores se situent près du centre et la courbe chute aux deux extrémités. Il reste à définir au moins une zone ou région de rejet. En voici deux ici, en rouge. Il s'agit des 2, % les plus bas, l'idée étant que cela pose un critère et que, si la valeur de l'échantillon tombe dans l'une de ces zones ou régions de rejet, il est très peu probable que cela arrive par hasard selon la distribution nulle. À partir de là, il va falloir prendre une décision. Le problème, quand on veut établir la présence d'un effet dans un échantillon, c'est qu'on peut se tromper. Notamment en cas de faux positif. J'entends par là que les données indiquent une sorte d'effet statistique, qui est pourtant, en réalité, dû au hasard. Le diagramme de dispersion, à droite, montre qu'il y a une association négative forte entre les deux variables. Sur l'axe X, de gauche à droite, les scores baissent. Comme vous le voyez, cette ligne de régression est relativement marquée. Cela dit, j'ai généré ces données aléatoirement, avec un code qui crée en réalité des variables non corrélées. Ce code était assorti d'une corrélation nulle et j'ai dû l'exécuter quatre ou cinq fois avant d'obtenir ce résultat au hasard. On a donc ici un faux positif. La corrélation est dans l'échantillon, mais pas dans la population concernée. Par ailleurs, on peut uniquement obtenir un faux positif si on rejette l'hypothèse nulle. C'est plutôt logique. C'est ce qui s'appelle une erreur de type un, l'idée étant d'accepter que la distribution nulle puisse comporter des valeurs extrêmes. On peut choisir le taux de probabilité à notre convenance. Le choix le plus courant est 5 %, autrement dit 5 % de risques de faux positifs si l'hypothèse nulle est vraie. Mais il est aussi possible d'obtenir un faux négatif. Les données semblent aléatoires, alors qu'il existe une différence constante entre les groupes ou une association. Et c'est le cas, ici. La ligne de ce diagramme de dispersion est pratiquement plate. Mais, en réalité, le code utilisé était assorti d'une corrélation positive de 0,25, ce qui est relativement élevé. J'ai dû aussi l'exécuter quatre ou cinq fois avant d'obtenir une association plate. C'est la preuve que la variation aléatoire peut donner une autre impression que celle que la population totale peut donner. Un faux négatif est possible uniquement si on ne rejette pas l'hypothèse nulle, quand le résultat est négatif. C'est ce qui s'appelle une erreur de type deux et contrairement aux faux positifs, qui offrent un choix, cette valeur est calculée en fonction de plusieurs facteurs. Cela dit, la vérification d'hypothèse fait l'objet de nombreuses critiques. Un point important est qu'il est extrêmement facile de mal interpréter la probabilité obtenue comme résultat. De plus, certains s'opposent à l'idée même d'une hypothèse nulle selon laquelle l'effet serait de zéro. Par ailleurs, l'interprétation peut se révéler biaisée par le recours à un critère particulier. Mais le plus important, c'est qu'on peut estimer qu'elle répond à la mauvaise question. Elle donne la probabilité des données en fonction de l'hypothèse alors qu'on veut plutôt l'alternative bayésienne, autrement dit la probabilité de l'hypothèse en fonction des données. Il existe bien des moyens de contourner ce problème, mais ils ne sont pas évoqués, en général. Que pouvons-nous en conclure ? La vérification d'hypothèse s'impose dans les décisions de sélection. Elle est très utile, en dépit de certaines critiques. De nombreuses recherches cruciales se sont appuyées sur la vérification d'hypothèse. Enfin, les méthodes bayésiennes, qui renversent les probabilités, et l'estimation, autrement dit les intervalles de confiance, peuvent remplacer ou compléter la vérification d'hypothèse. 


-----------------------------------------------
Vidéo : Définir un niveau de confiance
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:22            Définir un niveau de confiance
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
L'autre approche des statistiques inférentielles dont nous allons parler est celle des intervalles de confiance. Ce sont des méthodes qui essaient de mesurer directement l'effet statistique. Quelle est l'ampleur de la différence entre les moyennes de groupe ou de l'association entre les variables ? Il faut alors choisir un niveau de confiance. C'est vous qui le sélectionnez. 95 % s'avère le plus courant. Comme le démontrent ces cercles concentriques, on peut avancer pour rétrécir l'intervalle. Ou reculer pour obtenir une conclusion plus large. L'idée est que, plus le niveau de confiance est élevé – par exemple, s'il passe de 95 à 99 % –, plus l'intervalle sera large ou plus le cercle sera grand dans ce cas de figure particulier. Sachez que deux éléments sont inversement liés. Commençons par l'exactitude. Dans le domaine de l'estimation par intervalle, l'exactitude est synonyme de centrage sur la valeur vraie. Ici, l'intervalle de confiance est exact s'il contient la valeur vraie de la population. Cela conduit à une inférence correcte à propos de la population. L'exactitude est inversement liée à la précision, qui a un sens distinct. Elle désigne ici un intervalle étroit, donc une petite plage de valeurs probables. Elle fonctionne autrement que l'exactitude. Nous allons prendre un exemple. Nous avons ici une situation hypothétique qui implique des valeurs s'échelonnant de 0 à 100. Nous avons une ligne pleine à qui est dans ce cas la valeur vraie de la population. Cette distribution donne une plage de valeurs qui n'est ni exacte ni précise. Elle n'est pas exacte, car elle passe à côté de la valeur vraie, qui se trouve à droite de 50 %. Si c'était un sondage politique, on en déduirait la mauvaise réponse. Et elle n'est pas précise non plus, car elle est très dispersée. En revanche, cette distribution est exacte, car elle est centrée sur la valeur vraie de 55 % ; mais elle n'est pas précise, car elle est aussi très dispersée : on aurait deux tiers de chances d'obtenir la bonne réponse, soit un tiers de risques d'obtenir la mauvaise réponse. Là encore, surtout dans le cas d'un sondage politique où le seuil de 50 % est particulièrement critique. Par contraste, cette distribution est extrêmement précise. La plage de valeurs est étroite, sur environ 10 points de pourcentage. Mais elle n'est pas exacte. Elle est presque totalement de mauvais côté et passe donc à côté de la cible. L'idéal, ce serait ça. Une distribution à la fois exacte, car centrée sur la valeur vraie, et précise car extrêmement étroite. Voici les quatre versions sur un même écran, avec celles qui ne sont pas exactes à gauche et celles qui sont exactes à droite, la distribution idéale se situant en bas à droite. Ensuite, vient le moment de l'interprétation où vous expliquez les résultats au client. Le problème, avec les intervalles de confiance, c'est l'espèce d'écart qu'il peut y avoir entre le résultat statistique et son interprétation. Le résultat est assez facile à obtenir. Nous allons prendre l'exemple d'un intervalle de confiance de 95 % pour une moyenne compris entre 5,8 et 7,2. Ces valeurs sont fictives. L'interprétation en langage courant, quand on ne réfléchit pas trop ou qu'on n'étudie pas le cas attentivement, c'est qu'il y a 95 % de chances que la moyenne de population se situe entre 5, et 7,2. Mais l'approche type veut que les moyennes de population soient fixes. Or, l'interprétation courante laisse entendre qu'elles fluctuent. L'interprétation correcte serait donc plutôt que 95 % des intervalles de confiance pour les échantillons sélectionnés aléatoirement contiennent la moyenne de population, car ce sont les échantillons qui fluctuent et non la population. Nous allons le voir sous forme graphique. Voici 20 intervalles de confiance générés aléatoirement pour une moyenne de population de 55. Ils vont des valeurs faibles, en bas, aux valeurs élevées, en haut. Notez que 19 d'entre eux croisent la valeur vraie de population et sont donc des intervalles de confiance exacts. En revanche, l'intervalle numéro 18, sur la droite, apparaît en bleu parce qu'il passe complètement à côté. Cela arrive en raison de la variation aléatoire des échantillons et des intervalles de confiance qui en découlent. Voici une liste de facteurs pouvant affecter la largeur d'un intervalle de confiance. Déjà, le niveau de confiance. Plus il augmente, passant de 80 % à 90 %, puis à 95 % ou à 99 %, plus l'intervalle s'élargit. Ensuite, l'écart-type, autrement dit la variation qui est inhérente à l'objet de votre étude. Certaines choses fluctuent peu et les intervalles de confiance restent alors étroits. D'autres fluctuent beaucoup et leurs intervalles sont larges. Mais l'essentiel est la taille de l'échantillon. Un échantillon restreint va de pair avec un intervalle large. arbitrairement étroit. La taille d'échantillon est donc capitale pour obtenir des intervalles précis. Que pouvons-nous en conclure ? Les intervalles de confiance sont axés sur l'estimation directe des paramètres. Comme ils donnent une plage de valeurs élevées et faibles, la variation des données est explicitement incluse. Les intervalles de confiance fournissent donc davantage d'informations que les vérifications d'hypothèse. d'informations que les vérifications d'hypothèse. 


-----------------------------------------------
Vidéo : Aborder les problèmes en modélisation
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:02:05            <vAborder les problèmes en modélisation
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Penchons-nous sur les problèmes courants rencontrés en modélisation, c'est-à-dire lors de la création de modèles statistiques. Sachez que la science des données peut devenir complexe. Par exemple, dans le cas de la non-normalité des données. En général, on obtient des courbes en cloche ; toute autre forme révèle un problème. Ou en cas de non-linéarité dans l'association entre les variables. Les droites sont plus faciles à gérer que les autres formes, telles que les courbes. Ou en cas de multicolinéarité, que nous avons précédemment évoquée. C'est quand les prédicteurs sont associés et qu'il y a chevauchement entre leurs prédictions et la variable de résultat. Ou en cas de données manquantes, ce qui cause une série d'autres difficultés. Passons ces problèmes en revue. La non-normalité est problématique, car il faut des courbes en cloche. Voici une belle distribution normale, en cloche. Mais on peut avoir une distribution très biaisée. Ou bien une distribution mixte, qui peut être de forme bimodale, entre autres. On peut avoir des aberrations. De plus, nous savons que la non-normalité déforme les mesures et les modèles utilisés. Plusieurs possibilités s'offrent à vous. Vous pouvez essayer de recadrer légèrement une distribution asymétrique en vous efforçant de transformer les données. Vous pouvez aussi scinder une distribution mixte, ce qui devrait vous permettre d'obtenir deux distributions normales. La non-linéarité est synonyme d'association courbée. La plupart des approches, telles que la régression, tracent des lignes droites, comme ici. En revanche, les données dessinent ici une forme très courbée, voire une courbe parfaite. La linéarité est une supposition de base. En cas de problème, vous pouvez essayer de transformer une seule des variables, voire les deux, pour voir si la courbe se redresse. Si vous avez une courbe de croissance, vous pouvez introduire un polynôme avec une élévation au carré ou au cube, ou une autre fonction permettant de redresser la courbe. La multicolinéarité, dont nous avons déjà parlé, est problématique parce que les prédicteurs corrélés se chevauchent dans leur explication de la variable de résultat. L'ennui, c'est que cela peut déformer les coefficients. Certaines procédures sont moins concernées que les autres. Une option intéressante consiste à réduire le nombre de variables pour obtenir un modèle qui reste solide sur le plan prédictif. Mais surtout stable et se prêtant à l'interprétation. Par ailleurs, toute décision n'a pas à se fonder sur les données. La théorie permet aussi de faire votre choix parmi les variables possibles. C'est une des raisons pour lesquelles la science des données allie programmation informatique, maths et statistiques et expertise significative : la théorie permet d'orienter les analyses. Il existe deux autre problèmes, dont un que nous avons déjà évoqué : l'explosion combinatoire. C'est quand les combinaisons de variables et de catégories explosent. Par exemple, si vous avez seules 16 combinaisons sont possibles. Vous pouvez assez facilement toutes les explorer. Maintenant, imaginez 20 variables avec 5 catégories chacune, ce qui est très courant. Cela fait des milliards de combinaisons. Il devient inimaginable de toutes les explorer. Il faut donc trouver un moyen de réduire quelque peu la voilure. Là encore, la théorie, fondée sur l'expertise significative, peut très bien vous servir de boussole. Les méthodes de Monte-Carlo par chaîne de Markov sont couramment employées dans ce cas. C'est un sujet qui mériterait une formation à lui tout seul, mais disons que cette approche peut fournir une solution aux problèmes inextricables, quand il est impossible d'explorer toutes les combinaisons. Autre grand problème : la malédiction de la dimensionnalité. C'est quand le phénomène observé se produit uniquement dans des combinaisons de plusieurs dimensions et qu'il est donc hautement dimensionnel. Cela peut devenir difficile à expliquer et difficile à prédire : vous pouvez essayer de réduire la dimensionnalité ou, comme dans le cas de l'explosion combinatoire, recourir aux méthodes de Monte-Carlo par chaîne de Markov. Les données manquantes peuvent déformer les analyses et biaiser les résultats. Vous devez voir s'il existe des tendances particulières dans ces lacunes. Est-ce qu'il manque des valeurs dans un groupe de cas précis pour une variable ? Si oui, vous devez essayer de comprendre pourquoi elles manquent pour ces cas-là et pas pour les autres. Faute de tendances particulières, vous pouvez éliminer les cas manquants, mais c'est rare. Vous pouvez également imputer les valeurs manquantes. Il existe plusieurs méthodes pour cela, dont certaines marchent mieux que d'autres. Nous pouvons déduire quelques conclusions à l'issue de cette présentation. Déjà, et ce n'est surprenant, certaines données peuvent causer des complications lors de la modélisation. Il peut s'agir d'ambiguïté dans les réponses ou les modèles, de biais dus à des données manquantes ou de suppositions remises en cause par la non-normalité. Enfin, vous pouvez utiliser des méthodes analytiques ou la théorie significative dans votre branche pour régler certains de ces problèmes. 


-----------------------------------------------
Vidéo : Tenir compte de la validation des données
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:02:09            Tenir compte de la validation des données
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Un sujet qu'il est malheureusement facile d'ignorer en science des données est celui de la validation des modèles. Êtes-vous vraiment sur la bonne voie avec votre analyse ? Beaucoup d'algorithmes de machine learning échouent lors de leur mise en œuvre et beaucoup d'études ne sont pas reproductibles. Il faut donc vous assurer que votre analyse fournit des informations pertinentes sur la nature globale du problème, et non juste sur les données observées. Autrement dit, votre modèle est parfaitement adapté à l'échantillon. Formidable, mais conviendrait-il à d'autres données ? C'est la question de la généralisabilité ou de l'évolutivité. Une possibilité consiste à utiliser les probabilités postérieures, autrement dit à combiner des informations sur le présent et des informations sur le passé afin de se forger une sorte d'image de l'avenir. On obtient alors la probabilité des données en fonction de l'hypothèse. C'est la base des analyses, sauf qu'il est plus intéressant et plus utile de connaître la probabilité de l'hypothèse en fonction des données. Le théorème de Bayes, dont nous avons parlé précédemment, permet d'opérer ce renversement. La plus simple des méthodes de validation est la reproduction des études. Il s'agit de voir si on peut répéter une procédure en obtenant les mêmes résultats. La reproduction peut être soit exacte, soit conceptuelle. Une reproduction exacte se déroule à l'identique, tandis qu'une reproduction conceptuelle présente des variantes. On peut également combiner les résultats des études à l'aide des méta-analyses ou des méthodes bayésiennes. Ces deux options sont efficaces, car la reproductibilité est la règle d'or dans bon nombre de domaines. Citons ensuite la validation simple, qui consiste à prendre le jeu de données et à le scinder en deux afin de créer un modèle avec la première partie, puis de tester le modèle avec la seconde partie. Cette tâche toute simple requiert néanmoins un échantillon très large permettant d'ignorer la moitié des données jusqu'à la phase de test. C'est une méthode qui se rencontre souvent en concurrence analytique. Ensuite, la validation croisée. C'est quand on a les mêmes données pour l'apprentissage et le test. Une approche courante est le leave-one-out, ou tous sauf un. C'est l'idéal pour les modèles linéaires. Simple et rapide, il permet de calculer la fiabilité des résultats en laissant une valeur à la fois en dehors de l'échantillon. On le rencontre aussi sous le nom de jackknife. Il existe une variante, le leave-p-out, qui consiste à choisir une valeur particulière. Et il y a le k-fold, où les données sont scindées en partitions : un groupe est mis de côté pour servir de cas de test tandis qu'on modélise les autres, puis on fait tourner afin que chaque groupe serve de cas de test. Cette présentation conduit à un certain nombre de conclusions. D'abord, vous devez vous assurer que votre analyse compte vraiment et vous informe au-delà des données relatives à votre échantillon. Ensuite, vous devez contrôler la validité de vos conclusions ainsi que la généralisabilité de votre modèle. C'est important, car cela assoit la fiabilité à la fois de votre analyse et du modèle que vous avez créé. 


***********************************************
Chapitre : 9. S'initier au machine learning
***********************************************


-----------------------------------------------
Vidéo : Découvrir les arbres de décision
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:58            Découvrir les arbres de décision
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Sur le thème du machine learning, nous allons commencer par parler des arbres de décision. Les écologistes disent que les arbres sont la solution et, en machine learning, cela se vérifie souvent. Voici un exemple d'arbre de décision tout simple. Penchons-nous sur la composition de cette figure. Nous avons un nœud racine, qui est le point de départ, ici en haut, puis nous avons des ramifications représentées par les branches, qui s'appellent les arêtes en mathématiques. Chaque étape de décision forme un nœud. Enfin, nous avons les feuilles, ou nœuds terminaux, la dernière catégorie dans laquelle les données peuvent tomber. Il existe deux grands types d'arbre de décision. Les arbres de classification, qui se fondent sur des données quantitatives et catégorielles pour modéliser des résultats catégoriels, et les arbres de régression, qui se fondent sur les mêmes données mais pour modéliser des résultats quantitatifs. Comme la régression multiple, sauf qu'un arbre est plus facile à créer et interpréter. La création d'un arbre passe par une décision importante, à savoir la sélection de l'algorithme ou de la méthode de ramification. Vous avez le choix entre l'ID3, qui signifie Iterative Dichotomiser 3, le C5, qui signifie Classifier et qui est une extension de l'ID3, le CART, sigle de Classification And Regression Tree (très courant en choix universel), le CHAID, sigle de Chi-squared Automatic Interaction Detector, le MARS, sigle de Multivariate Adaptive Regression Splines et la méthode que nous allons utiliser : l'inférence conditionnelle. Il y a du pour et du contre pour les arbres de décision. Côté pour, ils sont flexibles et prennent en charge divers types de données sans beaucoup de préparation. Ils sont résistants aux violations des hypothèses. Ils sont faciles à interpréter, prennent en charge de larges jeux de données et offrent un modèle de type boîte blanche qui permet de suivre le processus par lequel tel cas tombe dans telle catégorie. Côté contre, ces modèles reposent souvent sur l'heuristique et les optima locaux, donc ils optimisent telle étape de décision au niveau de tel nœud, sans considérer l'arbre tout entier. Certaines méthodes sont enclines au surajustement, avec trop de branches peu généralisables. Quelques concepts comme le « ou exclusif », dit aussi OUX, la parodie ou les multiplexeurs sont difficiles à modéliser dans un arbre de décision, mais ils restent rares. Et il peut y avoir un biais en raison du choix préférentiel des variables qui ont le plus de niveaux. Si j'ai opté pour l'inférence conditionnelle, c'est parce qu'elle est moins sujette à ce défaut. Maintenant, voyons ensemble un exemple concret dans R. Je vais commencer par prendre le package nommé « party », qu'on peut installer sur l'ordinateur si ce n'est pas déjà fait, et puis je vais utiliser les jeux de données issus des bibliothèques intégrées… voilà qui est fait. Nous allons créer un arbre d'inférence conditionnelle de type arbre de classification, mais on peut aussi créer des arbres de régression. Nous utilisons le jeu de données iris, qui contient 150 observations pour trois variétés d'iris ainsi que quatre mesures différentes pour chacune d'elles. Cela fait assez peu de variables à traiter, mais l'exemple est facile à comprendre. Il suffit de créer l'objet ctree, l'arbre d'inférence conditionnelle, pour poser les calculs correspondants. Pour consulter le détail, on vient ici et on voit des branches au niveau de la longueur de pétale, puis de la largeur de pétale, puis de nouveau de la longueur de pétale. Cela dit, il reste plus facile de visualiser les résultats sous forme graphique. J'exécute la commande plot et j'agrandis ce volet. Voici la version intégrale de l'arbre de décision relatif à la classification de 150 observations concernant trois variétés d'iris. D'abord, la longueur de pétale. Les pétales courts tombent tous en bas à gauche chez les setosa. Les pétales longs sont ensuite étudiés selon le critère de leur largeur, puis encore une fois selon le critère de leur longueur. Les versicolor tombent dans le nœud 5 et les virginica se regroupent à droite. Le nœud 6 est plus ambigu, car il présente un mélange de quatre et quatre, signe d'un problème de classification. Si vous voulez le visualiser sous forme quantitative, vous pouvez créer une table. Revenons dans R pour lui demander une table qui présente les catégories prédites ainsi que les catégories observées. Les observées sont alignées en haut et les prédites sont énumérées sur le côté. Les setosa ont tous été correctement classifiés, un versicolor a été placé à tort parmi les virginica et cinq virginica ont été incorrectement classifiés, ce qui est révélateur d'une certaine confusion. Cela dit, dans l'ensemble, cet arbre de classification a bien fonctionné avec deux simples mesures. Que pouvons-nous retenir des arbres de décision ? Un, les arbres de décision sont flexibles et faciles à créer, ce qui est très pratique. Deux, choisissez un algorithme adapté non seulement aux données dont vous disposez, selon si elles sont catégorielles ou quantitatives, mais aussi à la question à laquelle vous essayez de répondre. Trois, attention au surajustement : préférez une méthode moins sujette à ce défaut. 


-----------------------------------------------
Vidéo : Comprendre les ensembles
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:31            Comprendre les ensembles
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
L'une des plus grandes avancées du machine learning, c'est l'utilisation des ensembles ou modélisation d'ensembles. Il s'agit en quelque sorte de la version statistique de la sagesse collective. Le principe consiste à combiner des estimations pour calculer la moyenne de nombreuses estimations de votre résultat particulier. C'est utile, car la combinaison de plusieurs estimations se révèle souvent plus exacte qu'une seule estimation. Par exemple, quand on demande à plusieurs personnes de deviner le nombre de billes dans un pot, étant donné que certaines vont le surestimer et que d'autres vont le sous-estimer, la moyenne des estimations est souvent la plus proche du vrai nombre. En matière de combinaison d'estimations, il y a une chose à retenir : la diversité aide et nous allons revenir sur ce point. Diverses méthodes permettent de combiner des estimations. Parmi les trois plus courantes, on compte le bagging, abréviation de boosted aggregating. Il utilise des jeux de données tirés au sort. On crée des modèles prédictifs à partir de ces données, puis on combine leurs prédictions par le biais d'une sorte de vote de la part de chaque modèle. Il y a également le boosting, où chaque classificateur applique une pondération plus forte aux erreurs du classificateur précédent. Il souligne les problèmes. Enfin, il y a le blending, aussi appelé stacking. Il consiste à utiliser un modèle de second ordre pour combiner les résultats des modèles de premier ordre : vous voyez que nous entrons dans les subtilités, mais c'est le bagging qui a tendance à se révéler le plus utile. Nous allons prendre un exemple dans R. J'utilise le package nommé « randomForest », qu'on peut installer sur l'ordinateur si ce n'est pas déjà fait. Je charge également le package intégré des jeux de données… voilà qui est fait. On voit ici quelques informations sur le package. Je descends, je reprends les données concernant les iris, et regardons les en-têtes qui s'affichent. Quatre mesures quantitatives, suivies de la variété d'iris. Maintenant attention, place à une étape importante, car nous allons scinder les données en deux partitions. Je crée un jeu d'apprentissage avec deux tiers des données et un jeu de test avec le tiers restant. J'indique ensuite un germe aléatoire afin de garantir la constance ou la reproductibilité des résultats. Une fois la scission effectuée, je crée les deux jeux de données. Voici le jeu d'apprentissage et voici le jeu de test. Ensuite, je crée la forêt aléatoire des arbres de décision proprement dite, en indiquant un germe aléatoire. Et j'utilise la fonction randomForest. J'exécute tout cela, avec un nombre total de 500 arbres. Et je calcule la proximité. Maintenant, étudions les résultats de la forêt aléatoire. D'abord, j'affiche les résultats de la table de classification. Tout commence par l'appel, donc la commande. Ensuite la classification, avec 500 arbres, puis l'essai de deux variables à chaque ramification. Le taux d'erreur très faible de 3, répertoriant les cas mal classifiés. Les setosa étaient faciles à distinguer, mais deux versicolor ont été prédits comme des virginica et deux virginica ont été prédits comme des versicolor. Alors, revenons à notre script. Créons la courbe des erreurs par nombre d'arbres. Je vais cliquer à cet endroit pour réaliser le tracé, puis agrandir ce volet. Plus cette ligne est basse, plus le nombre d'erreurs est réduit. Vous voyez que cette ligne n'est pas toujours stable : elle fluctue, car elle extrait et combine des données aléatoires, mais au-delà de 250 arbres, elle devient nettement plus lisse. C'est l'un des avantages de procéder à de nombreuses répétitions, malgré leur caractère aléatoire. Nous pouvons également nous intéresser à l'importance relative des variables de prédiction. J'utilise la fonction importance… la voici, puis je crée un graphique avec cette information, que vous voyez s'afficher ici et que je vais agrandir. Ici, la longueur de pétale était le prédicteur le plus important par ordre décroissant d'une mesure d'erreur commune : la mesure de Gini. Venaient ensuite la largeur de pétale, puis la longueur de sépale, et enfin la largeur de sépale, presque sans effet. On peut prendre ce modèle et l'appliquer aux donnés de test précédemment mises de côté. C'est ce que je fais maintenant afin d'obtenir une nouvelle table. Là encore, vous voyez que les setosa étaient faciles à classifier. C'est bon pour eux. Mais dans les colonnes suivantes, un des dix versicolor a été mal classifié et un des quatorze virginica a aussi été mal classifié. Ces résultats se révèlent cohérents par rapport à ceux des analyses précédentes réalisées sur les iris. Que retenir de la modélisation d'ensembles, notamment des forêts aléatoires ? Un, plusieurs estimations valent souvent mieux qu'une en matière d'exactitude. Deux, la diversité des données, de par leur sélection aléatoire, et la diversité des modèles donnent des résultats plus exacts. Trois, le caractère aléatoire de la sélection des cas et de la sélection des variables joue un rôle clé dans la garantie de cette diversité, rendant les ensembles de modèles plus prédictifs et plus exacts. 


-----------------------------------------------
Vidéo : Utiliser la méthode des k plus proches voisins (kPPV)
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:51            Utiliser la méthode des k plus proches voisins (kPPV)
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Une méthode de classification très courante est celle des k plus proches voisins. L'idée est tout simplement d'utiliser les cas voisins comme prédicteurs pour déterminer comment classifier un cas précis. La méthode des k plus proches voisins ou k-NN pour K-nearest neighbors en anglais, où k est le nombre de voisins, est un mode d'apprentissage basé sur des instances qui consiste à étudier les exemples voisins d'un cas précis. Comme cette méthode s'appuie sur des instances, et non sur des paramètres, elle est dite « paresseuse ». Cela signifie en réalité que l'algorithme est tout simple, facile à décrire sur le plan conceptuel, pas dur à mettre en œuvre et étonnamment efficace. La première étape pour déterminer des voisins consiste à définir une mesure de distance. De combien les divers cas sont-ils éloignés ? Si vous avez un certain nombre de variables de prédiction – désignons-le par la lettre j –, vous pouvez calculer la distance euclidienne dans un espace à j dimensions. Ça, c'est si vous avez des variables quantitatives. Si vos données sont catégorielles, vous pouvez utiliser une mesure de recouvrement : la distance de Hamming. Par ailleurs, vous devez faire attention à l'explosion combinatoire, autrement dit au fait qu'un grand nombre de variables se traduit par un grand nombre de dimensions. Avant de vous lancer dans les k plus proches voisins, limitez le nombre de dimensions à l'aide d'une analyse factorielle ou en composantes principales. Ensuite, il faut choisir la valeur k, autrement dit le nombre de voisins à utiliser. Plus on a de voisins, plus le modèle sera lisse et plus il permettra d'éliminer les irrégularités, le risque étant qu'il introduise également du bruit aléatoire. Cela augmente la probabilité de classification incorrecte, donc il faut faire des choix. Une possibilité consiste à pondérer les voisins en accordant plus de valeur aux plus proches et moins de valeur aux plus éloignés ; une autre possibilité consiste à utiliser l'une des nombreuses variantes des k plus proches voisins, comme les étendus (les ENN) ou bien les condensés (les CNN). De nombreuses options s'offrent à vous, mais prenons un exemple tout simple dans R. Ce que je vais faire ici, c'est reprendre les données sur les iris et utiliser une bibliothèque qui porte de nom de class. C'est pour classification. Je la charge, donc, et comme je veux l'associer au jeu de données iris, je charge aussi le package correspondant. Le premier réflexe est de jeter un œil aux données. Je vais afficher les six premiers cas du jeu de données iris. Si vos échelles de données sont différentes, donc si les plages de variables sont différentes, il est intéressant de les standardiser. L'idée est de les classer en plages similaires. Cela standardise les variables. Ici, comme les données sur les iris sont assez proches les unes des autres, ce n'est pas nécessaire, alors nous pouvons sauter cette étape. Comme précédemment, je crée un jeu d'apprentissage avec deux tiers des données et un jeu de test avec le tiers restant. Un germe aléatoire pour la reproductibilité. Maintenant, ce que nous allons faire, c'est diviser les cas pour les répartir parmi les quatre variables quantitatives, puis nous allons supprimer l'en-tête des variétés. C'est ce que je fais ici, d'ailleurs : c'est le sens de la mention de un à quatre en fin de commande. Voici nos jeux d'apprentissage et de test. Comme nous avons besoin des en-têtes pour la classification, enregistrons-les dans un jeu de données distinct. C'est fait. Maintenant, reste à créer le classificateur. Comme on peut choisir la valeur de k, mieux vaut opter pour un chiffre impair qui évite les résultats ex aequo et on peut essayer plusieurs valeurs pour voir ce qu'elles donnent. Nous allons faire différents essais. Voici le modèle prédictif et la fonction k-NN avec la valeur trois. Nous commençons donc par étudier les trois plus proches voisins et, pour contrôler la classification, nous allons créer cette table qui nous montre que les 17 iris setosa ont été classifiés correctement. En revanche, deux versicolor sur dix ont été mal classifiés et un virginica a été mal classifié. Répétons l'opération avec une autre valeur de k et comparons les résultats. Je repars en arrière pour changer le trois en un plus gros chiffre, par exemple neuf. Je réexécute cette commande. Je mets tout en surbrillance… et il ne reste plus qu'à recréer la table pour comparer les résultats. J'agrandis pour qu'on voie les deux tables, et vous voyez que la classification se révèle légèrement meilleure pour les versicolor, dont un seul est mal classifié. J'ai déjà fait ce genre d'expérience et je sais qu'augmenter la valeur k n'améliore en rien la classification de ces variétés d'iris et, comme l'a montré notre précédente expérience, nous savons qu'un certain niveau de classification incorrecte est inhérent à ce jeu de données, mais cela montre bien comment fonctionne cet algorithme. Que nous apprennent ces expériences sur la méthode des k-NN ou k plus proches voisins ? Déjà, les k-NN sont plutôt simples. C'est une méthode de classification non paramétrique assez facile à décrire. Elle consiste à regarder tout autour afin d'exploiter les données voisines. Cela dit, n'oubliez pas que le choix de la valeur k et celui de la mesure de distance affectent les résultats obtenus, alors étudiez bien vos options et comparez les résultats afin de prendre la meilleure décision pour vos données. 


-----------------------------------------------
Vidéo : Aborder les classificateurs bayésiens naïfs
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:40            Aborder les classificateurs bayésiens naïfs
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Un autre choix courant parmi les algorithmes est celui des classificateurs bayésiens naïfs. Disons qu'ils illustrent l'efficacité déraisonnable de la naïveté, ou des solutions simples. La formule « bayésiens naïfs » impose une explication. Ce sont des classificateurs, donc ils répartissent les cas d'un jeu de données en diverses catégories. Ils sont bayésiens, car ils se fondent sur le théorème de Bayes, qui consiste à calculer la probabilité d'une certaine classe en fonction des données. Et ils sont naïfs, car ils ignorent les relations entre les prédicteurs. Cela peut sembler problématique, mais curieusement, cette méthode s'avère extrêmement efficace. Malgré leur naïveté, de par l'ignorance des relations entre les variables, ces classificateurs fonctionnent très bien, et encore mieux avec la préparation des données. Ils permettent d'équilibrer les tailles de classe. Ils permettent de standardiser les pondérations pour compenser l'interdépendance des fonctionnalités. Ils permettent de transformer les données afin d'émuler la distribution d'une fonction de puissance. D'autres options sont envisageables, mais ce sont là celles qui contribuent le plus à l'efficacité de cette méthode. Prenons un exemple de la classification bayésienne naïve dans R. Pour cela, nous utiliserons deux packages externes. Le premier s'appelle e1071 : c'est le nom de la classe pour laquelle il a été développé. Il comporte l'approche bayésienne qui nous intéresse. Le second s'appelle mlbench en référence aux benchmarks – c'est-à-dire aux critères – de machine learning. On les installe, au besoin, et puis on les charge. Prenons le jeu de données HouseVotes84, qui contient les votes de la Chambre des représentants (autrement dit des députés) des États-Unis en 1984. On charge ces données et on affiche les six premiers cas qui en ressortent. Je vais agrandir un peu ce volet. Nous obtenons ici la classe du parti politique de six députés américains ainsi que 16 variables, de V1 à V16, qui représentent leurs votes dans le cadre de 16 projets de loi. La mention &lt;NA&gt; désigne une abstention ou une absence. Cela nous fait des oui et des non, que nous allons utiliser pour essayer de classer ces députés dans le parti démocrate ou républicain. Alors, revenons à notre modèle. Comme précédemment, je divise les données en deux jeux distincts : un d'apprentissage et un de test. J'ajoute le germe pour la reproductibilité, je divise les données en deux, puis je crée le jeu d'apprentissage d'un côté et le jeu de test de l'autre. Je viens ici, plus bas, créer le classificateur. Pour cela, je crée l'objet nbc, pour naive Bayes classifier, qui est le nom anglais du classificateur bayésien naïf, et je précise les données à exploiter. Je vais agrandir ce volet pour qu'on y voie mieux ce qui s'y passe. Tout d'abord, nous avons les probabilités antérieures, ou a priori, qui sont les pourcentages relatifs à notre échantillon d'apprentissage. Nous avons 59 % de démocrates et 41 % de républicains. Puis, les probabilités conditionnelles indiquent les pourcentages de démocrates et de républicains qui ont voté oui ou non pour chacun de 16 projets de loi. Par exemple, pour le premier, V1, 36 % des démocrates ont voté non et 64 % ont voté oui, tandis que 18 % des républicains ont voté oui et 82 % ont voté non. Vous voyez que pour le deuxième, V2, c'est presque du 50/50 des deux côtés, tandis que certains autres résultats sont très contrastés. Nous pouvons alors prendre ces cas des données d'apprentissage pour essayer de bien les classifier en fonction du parti politique. Mais, ce que je vais faire maintenant, c'est contrôler le fonctionnement de ce modèle avec le jeu d'apprentissage. Pour cela, je crée une table, et vous voyez que la classification laisse un peu à désirer. Laissez-moi juste traduire ces valeurs en proportions en utilisant la commande prop.table, avec multiplications et résultat arrondi. Vous voyez que 94 % des démocrates et 88 % des républicains ont été classifiés correctement dans le jeu d'apprentissage. Faisons pareil dans le jeu de test. Il suffit de créer une nouvelle table : je reprends le même modèle nbc – qui est, pour rappel, le classificateur bayésien naïf – mais je l'applique cette fois aux données de test. Une fois encore, la classification se révèle imparfaite. Voyons les pourcentages avec la fonction d'arrondi. Dans le jeu de test, nous avons 96 % des démocrates et 77 % des républicains classifiés correctement en fonction de leur historique de vote à la Chambre des représentants. Naturellement, il est possible d'élargir l'échantillon. Cela donnerait davantage d'observations et plus d'exactitude, mais cet exemple illustre déjà les principaux rouages des classificateurs bayésiens naïfs. Quelles conclusions pouvons-nous tirer de cela ? Un, la méthode bayésienne naïve est simple, mais très efficace. Deux, elle marche avec divers prédicteurs. Vous pouvez avoir des variables quantitatives et catégorielles. Trois, les résultats sont faciles à interpréter. C'est donc un excellent choix pour la classification en matière de machine learning. 


-----------------------------------------------
Vidéo : Définir les réseaux neuronaux artificiels
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:25            Définir les réseaux neuronaux artificiels
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Les réseaux neuronaux artificiels sont des outils ou algorithmes particulièrement intéressants en machine learning. Leur nom pourrait faire penser qu'ils visent à reconstituer un cerveau, alors que leur but consiste plutôt à suivre l'exemple du cerveau. Les réseaux neuronaux artificiels s'inspirent d'ailleurs de leurs pendants biologiques : des neurones de données, plus précisément des neurones algorithmiques, échangent des informations par le biais de connexions qui s'adaptent ou pondèrent sur la base de l'expérience. Ils se rencontrent parfois dans les cas où l'apprentissage basé sur les règles ne marche pas, ne va pas jusqu'au bout ou ne permet pas une bonne description. C'est le cas pour la vision par ordinateur, pour l'identification photo ou vidéo, la reconnaissance vocale ainsi que toutes les inférences de fonction non linéaire difficile à décrire. Nous avons ici une espèce de modèle de type boîte noire, avec des entrées, un processus de traitement qui se déroule à l'intérieur de la boîte noire, qui est ici le réseau neuronal, et des sorties. Prenons un exemple simple de réseau neuronal. Nous allons utiliser un package nommé « neuralnet », qu'on peut installer sur l'ordinateur si ce n'est pas déjà fait, et nous allons le charger. Puis, nous allons préparer les données. Nous allons poser à ce réseau neuronal un problème tout simple à résoudre, à savoir comment traiter les racines carrées. C'est important, car il s'agit d'une fonction non linéaire ; par ailleurs, nous n'allons pas utiliser la fonction de racine carrée intégrée, puisque nous la réservons pour plus tard, et nous n'allons pas non plus lui fournir tous les exemples possibles : nous allons voir s'il peut déduire une tendance via la simple analyse du jeu d'apprentissage. Nous allons donc devoir commencer par générer des données aléatoires. On prend un germe aléatoire, qui garantit la reproductibilité, puis on crée un échantillon de 50 nombres aléatoires de 1 à 100 avec remplacement, c'est-à-dire qu'un même nombre peut apparaître plus d'une fois. Voyons les six premiers de ces 50 nombres : d'abord deux 41, suivis d'un 94, etc. Du côté des sorties, nous allons cette fois recourir à la fonction de racine carrée intégrée, appliquée à notre y d'apprentissage, y désignant la variable de résultat. Vous voyez ici les six premiers nombres qu'on obtient. Rapprochons les deux pour que les résultats soient plus lisibles et faciles à traiter. J'utilise la commande as.data.frame, qui donne ces six premières valeurs. On a 41 avec une racine carrée de 6,40, encore 41, puis 94 avec une racine carrée de 9,69, etc. Il faut maintenant former le réseau neuronal. Nous devons préciser le nombre de couches, ou plutôt de neurones, à utiliser – disons 10 dans cet exemple – et nous devons indiquer un seuil pour que le réseau sache où s'arrêter. Cela vient de la dérivée partielle de la fonction d'erreur et, dans ce cas, je définis cette valeur sur 0,01. On exécute la fonction de racine carrée nommée net.sqrt, puis on réalise le tracé correspondant. Cela dessine une figure, que je vais agrandir, car elle est un peu dure à interpréter. Nous avons les sorties d'un côté, les pondérations qui descendent aux neurones, des interconnexions au milieu, puis de nouveau les pondérations qui ramènent le tout en essayant de prédire les valeurs de la variable de résultat train.y. Nous pouvons tester ce réseau, tout juste développé, avec d'autres données. Pour cela, on crée un jeu avec des chiffres et des nombres dont nous savons que la racine carrée est un entier, donc on élève la plage numérique de 1 à 10 au carré. Ensuite, on fait passer ces notre réseau neuronal artificiel. Demandons-lui de reprendre le même modèle et voyons ce qu'il nous donne. Je vais agrandir un peu ce volet. Les sorties sont nombreuses. On a ici les entrées initiales. Vous voyez une colonne de 4, 9, etc. de l'autre – ce sont les carrés –, puis toute une série de calculs de pondérations pour les neurones artificiels, que je ne pense pas que quiconque lise en entier de toute façon, tant elle est longue. Nos résultats figurent à la fin : c'est ce que ce réseau prédit à partir de nos entrées. Il ne reste plus qu'à comparer ces valeurs à nos attentes. Nous allons donc créer une table qui contient la valeur x initiale, les racines carrées et la version arrondie du résultat du réseau neuronal. Ajoutons des en-têtes aux colonnes et affichons la table. Je vais agrandir un peu cela. Vous voyez que, même sans exemple de toutes les racines carrées possibles, ce réseau donne des résultats très proches de la réalité. Il tombe même très souvent juste, à une ou deux décimales près. Le plus gros écart est celui de la deuxième ligne, où il estime que la racine carré de 4 est 2,04, mais cela nous apprend qu'il est capable de déduire une règle complexe à partir d'un jeu d'apprentissage incomplet. C'est important pour l'inférence d'une tendance linéaire, comme ici. Que pouvons-nous conclure de cela ? Trois choses. Un, les RNA sont capables de déduire des règles complexes. Les racines carrées sont simples, mais la vision par ordinateur est plus ardue. Deux, ils se forment par l'expérience et s'adaptent en modifiant les pondérations entre leurs connexions. Trois, ils reposent sur une boîte noire, avec un processus opaque difficile à comprendre et donc difficile à expliquer. Cela dit, vu sa complexité, il n'est pas nécessaire de se plonger dans les détails : il suffit de voir si les résultats sont exacts, ce qui était plutôt le cas dans notre exemple. 


***********************************************
Chapitre : 10. Gérer la communication
***********************************************


-----------------------------------------------
Vidéo : Maintenir l'interprétabilité
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:29            Maintenir l'interprétabilité
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Vous devriez toujours garder l'interprétabilité à l'esprit lorsque vous menez un projet de science des données. Je m'explique. Il faut être un narrateur orienté données. Vous devez résoudre des problèmes pour obtenir une valeur. Vous recherchez la valeur des données. Le problème, c'est que l'analyse seule ne représente aucune valeur. Pour moi, voici les termes de l'équation. L'analyse fois le récit, autrement dit votre axe narratif, égalent la valeur. Notez que cette relation est une multiplication. Il ne s'agit pas d'une addition, donc, en suivant cette logique jusqu'au bout, si vous avez zéro récit, vous obtiendrez zéro valeur à la fin de votre projet. Vous devez vous efforcer de maximiser le récit ou l'axe narratif, donc l'interprétabilité de votre projet, afin de maximiser la valeur à tirer de vos données et de votre analyse. Pensez à vos objectifs. L'analyse doit être orientée objectif. Le récit proposé, votre axe narratif, doit correspondre aux objectifs visés. Vous devez notamment veiller à répondre sans aucune ambiguïté aux questions que vous ou votre client pouvez vous poser. L'essentiel est de ne pas oublier que votre client n'est pas vous. Dans ce but, vous avez plusieurs écueils à éviter. L'égocentrisme, notamment. Je ne parle pas ici de la vanité ni même de l'égoïsme. Je parle du phénomène psychologique qui conduit à penser que les autres voient, savent et sentent ce que nous voyons, savons et sentons. Il existe aussi un risque de faux consensus, quand on pense que tout le monde est au courant et qu'on peut donc se passer d'explication. Il y a également le problème de la fixation, quand quelqu'un a une idée en tête et qu'il est difficile de l'en détourner. Par ailleurs, comme votre client n'est pas vous et que vous devez communiquer, votre axe narratif doit suivre une série d'étapes claires. Voici quelques solutions à ce problème. Lors de votre présentation au client, commencez par énoncer la question, c'est-à-dire ce qui a motivé l'analyse. Donnez votre réponse en prenant soin de l'exposer clairement. Nuancez au besoin et procédez par ordre. Un autre point important est la question de décrire en détail ou non au client le raisonnement que vous avez suivi. Ne le faites que rarement, et juste pour faciliter l'interprétation. L'idée est que l'analyse est un exercice de simplification. Deux citations le confirment. D'abord : « Tout devrait être aussi simple que possible, mais pas plus simple. » Elle est d'Albert Einstein. Ensuite : « Moins, c'est mieux. » Elle est souvent attribuée à Ludwig Mies van der Rohe, mais elle est du poète anglais Robert Browning. Vous devriez garder ces deux concepts à l'esprit lorsque vous construisez l'axe narratif de votre analyse. Vous avez le choix. Un, optez pour plus de figures et moins de texte. Deux, simplifiez toutes les figures utilisées. Trois, évitez les tables et les nombres. Cela finit par être trop lourd. Quatre, même dans vos figures, mettez moins de texte. Il vaut mieux, là encore, accentuer les aspects visuels. Tout cela vous aidera à vous concentrer sur la narration d'un récit qui a du sens pour votre client. Nous allons en voir ensemble un exemple. C'est la célèbre affaire des admissions à l'université américaine de Berkeley. Voici le problème. En 1973, 44 % des 8 000 et quelques candidats masculins à l'université de Berkeley ont été admis, tandis que 35 % des 4 000 et quelques candidates féminines ont été admises. Cette différence a donné lieu à un procès retentissant, qui illustre l'idée du biais du genre dans les admissions universitaires. Or, rien n'est moins sûr. En effet, regardez ce que donnent les résultats ventilés par filière. Nous avons ici six filières, qui portent tout simplement les lettres A, B, C, etc. À y regarder de plus près, dans la A, le taux d'admission est supérieur pour les femmes. Il en va de même dans la B, de même dans la D et de même dans la F. Ainsi, quatre des six filières ont plus souvent admis des femmes que des hommes. Qu'est-ce que cela signifie ? C'est l'illustration de ce qui s'appelle le paradoxe de Simpson et que je me représente comme le retrait des couches successives d'un oignon. Expliquons tout ça. Le biais est négligeable au niveau des filières et peut-être favorable aux femmes. Ce qui se passe, c'est que les femmes se présentent en plus grand nombre aux filières les plus sélectives. Certains arrêtent l'analyse à ce stade, mais l'axe narratif n'est alors ni satisfaisant ni exhaustif, car l'histoire est loin de s'arrêter là. Il y a encore plusieurs questions à se poser. Pourquoi la taille de la classe varie par filière ? Pourquoi l'une admet plus que l'autre ? Pourquoi les taux d'admission fluctuent d'une filière à l'autre ? Et pourquoi les taux de candidature sont différents chez les hommes et les femmes ? Ce sont autant de questions d'approfondissement à poser pour mieux explorer le phénomène que vous étudiez. Il existe aussi d'autres éléments à prendre en considération. Quels sont les critères d'admission de chaque filière ? Comment chacune fait-elle sa promotion ou son recrutement ? Quelle formation scolaire ont les candidats et candidates à chacune des filières ? Enfin, un point clé : quel est le niveau de financement de chaque filière ? Si vous voulez savoir s'il existe ou non un biais de genre ou comprendre comment les filières fonctionnent, vous devez voir au-delà des simples chiffres et pourcentages. Vous devez les replacer dans leur contexte et exposer toute l'histoire du phénomène observé. Que peut-on conclure de ça ? Un, l'analyse a besoin du récit. Il faut un axe narratif pour avoir de la valeur aux yeux du client. Deux, traitez les objectifs et les questions de votre client, Il ne faut pas couper court : faites ce qu'il faut, Il ne faut pas couper court : faites ce qu'il faut, mais de façon aussi claire et succincte que possible. mais de façon aussi claire et succincte que possible. 


-----------------------------------------------
Vidéo : Fournir des informations exploitables
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:01:08            Fournir des informations exploitables
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Lorsque vous présentez les résultats d'un projet de science des données, vous devez fournir des informations exploitables au client. Pour être plus clair : les données servent en fait à agir. C'est une paraphrase d'une citation de l'un de mes héros : « Ma pensée est d'abord, enfin et toujours tournée vers mon action ». Elle est du philosophe et psychologue américain William James. L'idée est qu'il ne sert à rien de travailler dur lorsqu'on ne sait pas ce qu'on fait et lorsqu'on ne dispose d'aucun élément utile. Le baseballeur Yogi Berra, lui, a une formule originale : « On est perdu, mais on est dans les temps ». L'idée est qu'il faut pouvoir indiquer le chemin. N'oubliez pas que tout projet a sa motivation. Pourquoi a-t-il été réalisé ? Tout projet vise habituellement à orienter une action future. On cherche toujours à prendre une décision ou une autre. L'analyse que vous effectuez devrait justement guider cette action de façon à la fois censée et éclairée. Dans cette optique, vous devez être en mesure de préciser les prochaines étapes au client. Annoncez la suite des évènements, en justifiant vos recommandations par les données analysées. Soyez aussi précis que possible et veillez à ce que les recommandations que vous formulez soient effectivement applicables par votre client : chacune doit prendre le relais de la précédente, étape après étape. Attention : il existe une complication majeure qui a trait à la différence entre corrélation et causalité. La corrélation désigne la simple association de deux variables, tandis que la causalité désigne l'influence directe d'une variable sur l'autre. Voici votre problème. Vous avez de la corrélation, car les données sont intrinsèquement corrélationnelles, mais votre client veut de la causalité afin de savoir si tel choix produira tel résultat. Il y a donc un décalage. La question est : comment passer de la corrélation (qui est ce que vous avez) à la causalité (qui est ce que veut votre client) ? Ce n'est pas une interrogation philosophique sur l'ontologie ou l'épistémologie, mais une question pratique. On peut d'ailleurs y apporter une réponse pratique. Il existe plusieurs options conduisant à la causalité. Vous pouvez réaliser une étude expérimentale, avec essai contrôlé randomisé. En théorie, ce serait l'approche la plus simple. Elle se révèle parfois très dure en pratique et il faut la mettre en place avant la collecte des données. Vous pouvez aussi procéder à des quasi-expériences. Il s'agit de méthodes basées sur des données non randomisées, donc corrélationnelles. Elles sont fréquemment utilisées dans l'enseignement, en économie et en épidémiologie, mais aussi parfois dures à mettre en pratique. Enfin, il y a la théorie et l'expérience. La théorie basée sur les recherches et l'expérience spécifique d'un domaine. Ce sont des formes implicites de calcul de pari qui permettent de se fonder sur son expérience et trouver un sens dans un contexte précis. Réfléchissez aussi aux facteurs sociaux et contextuels de vos recommandations. Revoici, pour rappel, le diagramme de Venn de la data science, avec ses trois grandes branches : la programmation, les statistiques et mathématiques ainsi que l'expertise d’un domaine. Il se trouve que certains ont suggéré l'ajout d'un quatrième disque à ce diagramme. Il s'agit de la conscience sociale, autrement dit la compréhension des interactions. Voyons ensemble comment cela peut fonctionner. Vos recommandations doivent correspondre à la mission de votre client. Toute entreprise a un énoncé de mission. Cela définit explicitement le contexte dans lequel placer vos recommandations. Par ailleurs, vos recommandations doivent correspondre à l'identité de votre client. Certaines suggestions qui semblent cadrer avec la mission du client et maximiser ses résultats échouent sur ce point, comme la recommandation de faire payer des services religieux. Ayez conscience de l'environnement commercial dans lequel évolue le client. Informez-vous sur les réglementations existantes afin de lui offrir un avantage concurrentiel conforme à ses exigences juridiques et stratégiques. Enfin, ayez conscience de l'environnement social. Vos recommandations affecteront-elles les relations au sein de la société cliente ? Tout dépend des stratégies internes. Dans la mesure du possible, tâchez d'en avoir connaissance avant de formuler vos recommandations. Que pouvons-nous en conclure ? Votre analyse doit correspondre aux objectifs du client. Vous devez annoncer la suite en fonction de votre analyse. Vous devriez également avoir conscience du contexte social, et même culturel, dans lequel elle sera mise en œuvre. 


-----------------------------------------------
Vidéo : Développer la visualisation aux fins de présentation
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:19            Développer la visualisation aux fins de présentation
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
La visualisation aux fins de présentation est une autre compétence à développer en science des données. L'idée est que les images valent plus que les mots, qui valent eux-mêmes plus que les chiffres. La finalité de vos graphiques varie en fonction du stade de votre projet. Les graphiques exploratoires, dont nous avons parlé précédemment, se situent au rendez-vous de la rapidité et de l'interaction. Ils visent à permettre à l'analyste que vous êtes de parcourir les données pour voir ce qui s'y trouve. En revanche, les graphiques de présentation doivent être clairs et denses, car ils servent à communiquer vos conclusions au client. Vous devriez avoir vos priorités à l'esprit au moment de créer des visualisations. Préférez les graphiques au texte, aux tables et aux chiffres, mais sélectionnez les informations qui y figurent. Il est inutile de tout montrer : limitez-vous aux éléments pertinents pour le récit qui vous intéresse. Veillez d'ailleurs à construire votre axe narratif de sorte que chaque étape succède naturellement à la précédente et la complète. Et facilitez la compréhension. Un graphique en 3D, interactif, avec des quantités d'axes devient si élaboré qu'il perd sensiblement en lisibilité. Beaucoup de graphiques lisibles valent mieux que peu de graphiques illisibles. Mes conseils en matière de visualisation coulent de source. Libellez vos axes pour que vos graphiques soient clairs. Montrez toute la plage des données, sous peine de mal représenter les variations dans le temps ou entre les groupes. Évitez également la fausse 3D. Elle déforme les données et détourne l'attention des vraies informations. Enfin, sériez les éléments graphiques intuitivement, dans l'ordre dans lequel le client s'attend à les voir présentés. Je vais vous donner un exemple à ne pas suivre. Voici un graphique des niveaux de scolarité aux États-Unis, que j'ai créé sous Excel. C'est une compilation d'erreurs que j'ai déjà constatées : je n'ai fait que composer un florilège. On note bon nombre de problèmes. Par exemple, à gauche, l'échelle est microscopique et ne va que de 8 à 28, ce qui n'est pas l'intégralité de la plage de données. Les niveaux de scolarité suivent l'ordre alphabétique, et non l'ordre des cycles d'enseignement. Les cônes qui représentent les données sont durs à lire, avec leurs textures et couleurs équivoques. Certains sont tronqués, voire inexistants. En plus, les images et couleurs d'arrière-plan sont obscures et il n'y a aucune ligne de repère. Bref, c'est une cacophonie, un vrai chaos. Il vaudrait mieux créer une figure de ce genre. Les données sont identiques. Sauf qu'ici, les niveaux de scolarité suivent l'ordre chronologique ; de plus, la plage des taux commence à zéro et s'étend assez pour englober toutes les données. Ce graphique plus logique nous informe de façon plus claire et plus efficace. Le camembert est un choix populaire. Celui-ci représente nos données sur les niveaux de scolarité, avec les paramètres Excel par défaut, d'où son aspect quelque peu étrange. Malgré sa popularité, le camembert pose problème : dans chacun de ces trois-là, sauriez-vous dire si C est supérieur à D ou inversement ? On a C et D ici à gauche, ici au centre et ici à droite. C'est bien difficile à dire. En revanche, avec des graphiques à barres, on voit facilement si C est supérieur à D ou inversement. Cela devient évident. C'est une des raisons pour lesquelles les camemberts ne sont pas conseillés. Ils sont durs à lire. Les barres sont plus simples, plus lisibles et garantissent une communication efficace. L'idée à retenir est que, comme en conception industrielle, il vaut presque toujours mieux simplifier. Étudions quelques exemples. Je reprends ici des graphiques déjà vus. En voici un à barres, tout simple et gris avec une ligne de repère orange qui indique les informations souhaitées. Revoici celui des intervalles de confiance, très net, tout simple, qui montre clairement qu'un élément se distingue des autres. Voici un diagramme de dispersion que j'ai créé pour un autre projet, avec une aberration libellée. Et revoici la matrice qui regroupe plusieurs graphiques afin de communiquer clairement les informations qu'ils portent, au lieu de superposer toutes ces figures confusément. Nous avons aussi déjà vu celle-ci dans la vidéo sur le théorème de Bayes, ces neufs courbes distinctes se révélant plus lisibles que si je les avais superposées dans un même graphique. En conclusion, comme je l'ai dit au début de cette vidéo, les graphiques informent davantage que les mots, qui eux-mêmes sont plus faciles à manier que les chiffres. Vous devriez miser sur des visualisations à la fois claires, denses et simples. Et n'oubliez pas que vous devez construire un récit, donc tout un axe narratif, avec vos graphiques. 


-----------------------------------------------
Vidéo : Intégrer des recherches reproductibles
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:33            Intégrer des recherches reproductibles
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
recherches reproductibles. Elles consistent à laisser une trace numérique de son travail. Cela se révèle très intéressant pour diverses raisons. Un, cela permet de contrôler votre travail et vérifier vos conclusions. Deux, cela permet à votre client ainsi qu'aux futurs chercheurs et à vous-même ou à quiconque reprendrait le projet de voir comment il s'est déroulé. Trois, c'est imposé par certaines stratégies. Et quatre, la consignation du processus garantit une forme d'honnêteté intellectuelle. Vous allez devoir inclure plusieurs éléments dans vos recherches reproductibles. D'abord, toutes vos sources. Répertoriez les données brutes, donc pas encore traitées. Indiquez aussi la liste des objectifs du projet ainsi que le raisonnement suivi et les ressources utilisées : les logiciels, le matériel et même le personnel impliqué, les sources de financement et tout ce qui a pu intervenir. Ensuite, le processus mis en œuvre. Précisez le code que vous avez utilisé pour analyser les données. Expliquez les pistes explorées à chaque étape. Tenez un carnet de bord ou un journal de laboratoire. Enfin, le résultat obtenu. Spécifiez le jeu de données propre final avec lequel vous avez travaillé. Incluez des graphiques de présentation ainsi que l'intégralité des rapports élaborés afin de couvrir toutes les phases du projet de bout en bout. Le choix du niveau de détail s'apparente un peu à une recette. Que nous faut-il ? Assez de détails pour qu'en reproduisant chaque étape de l'analyse et en prenant les mêmes décisions, tout tiers parvienne au même résultat. Vous devez permettre aux autres de suivre le cheminement mental ainsi que le processus décisionnel que vous avez adoptés. Cela passe par de la documentation. Écrivez votre récit dans un journal de laboratoire. Je ne vous conseille pas les blocs-notes. En général, un enregistrement numérique est plus durable. Quoiqu'il en soit, consignez tout cela. Veillez également à commenter suffisamment le code utilisé afin que tout tiers comprenne son contenu, son fonctionnement et les raisons de sa sélection. Cela semble évident, mais n'oubliez pas non plus de donner un titre adéquat à vos fichiers et dossiers pour qu'ils soient identifiables. Enfin, sachez qu'il est préférable de construire votre axe narratif et tenir votre carnet de bord au fil de l'eau, au lieu de tout reconstituer mentalement à la fin. Je vous suggère deux autres principes si vous voulez vous faciliter la vie. Un, visez la portabilité. Évitez les formats propriétaires, si possible. Préférez les formats universels. C'est le cas, par exemple, des fichiers CSV ou texte et des documents PDF ou Markdown. Deux, parez au principe d'obsolescence. Préparez-vous à voir les logiciels évoluer, le matériel changer, les liens web se briser, vos sources disparaître et les participants au projet quitter l'entreprise. Donnez assez de détails pour que tout tiers puisse retrouver le fil même dans ces cas-là. Conclusion : aucun projet n'est isolé. En data science, il y a toujours un contexte et un lien avec d'autres projets. Indiquez assez de ressources et de détails concernant votre processus pour que d'autres puissent le reproduire. Enfin, anticipez les changements et préparez-vous à l'inévitable Enfin, anticipez les changements et préparez-vous à l'inévitable afin de fournir à votre client un produit aussi utile que durable. afin de fournir à votre client un produit aussi utile que durable. 


***********************************************
Chapitre : Conclusion
***********************************************


-----------------------------------------------
Vidéo : Aller plus loin avec la data science
-----------------------------------------------
Heure de la note :    Texte de la note :             

0:00:26            Aller plus loin avec la data science
La sélection de lignes de transcription dans cette section vous redirigera vers l’horodatage de la vidéo
Maintenant que vous cernez mieux le domaine de la data science, vous aimeriez sans doute savoir quoi faire pour acquérir et mettre en pratique de nouvelles compétences en la matière. La meilleure suggestion que je puisse vous faire est de vous investir. Pour cela, vous avez l'embarras du choix. Vous pouvez assister à des salons professionnels. Par exemple, la conférence internationale Strata + Hadoop World. C'est O'Reilly Media qui l'organise. Elle se tient plusieurs fois par an dans différentes villes du monde. Autre exemple : les rencontres Predictive Analytics World, qui ont lieu huit à dix fois par an à travers le monde. Il existe des conférences de moindre envergure, comme Extract, organisée par import.io, et Visualized, dédiée à la conception, la visualisation, la mise en récit et la technologie. Ou des évènements spécialisés, comme PyData et useR! Ils sont axés sur les communautés d'utilisateurs de Python et R. Bien sûr, de nombreux blogs sont disponibles en ligne. Le principal est peut-être DataTau. Il y a aussi R-bloggers, qui compile des informations sur R tirées de nombreuses sources. Notez également les blogs O'Reilly Radar, Open Knowledge et Planet SciPy. Je les consulte tous régulièrement et j'apprécie grandement leur conseils. Cela dit, le mieux est de se lancer dans la science des données humanitaire. C'est une démarche à la portée de tous dès le début. Par exemple, il y a DataKind, qui organise des projets de poids véritablement importants dans une optique d'assistance humanitaire. DataLook fournit le même genre de services et peut même vous aider à répliquer à votre échelle locale des grands projets qui ont été menés ailleurs. La Data-Pop Alliance est une autre source précieuse, car elle réunit des groupes de spécialistes et de philanthropes. Enfin, vous pouvez vous engager dans les Meetups Data for Good ou auprès d'autres organisations locales qui proposent des missions : il suffit de regarder autour de vous. Pour conclure cette vidéo, et cette formation, je dirais que la data science permet à la fois de gagner sa vie et de changer les choses. Merci de votre attention et bonne continuation. 

